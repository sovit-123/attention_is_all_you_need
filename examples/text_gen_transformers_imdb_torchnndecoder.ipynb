{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7eaf129",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Find small single text files for **Language Modeling** experiements here ⬇️\n",
    "\n",
    "https://www.kaggle.com/datasets/sovitrath/text-generation-language-modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab67365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tiktoken\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from utils.text_gen import get_batch, train_step, val_step, NLPDataset\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c113ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67a64d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 26 11:00:46 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   42C    P8    26W / 370W |    524MiB / 10009MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1293      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    0   N/A  N/A      1881      G   /usr/lib/xorg/Xorg                310MiB |\n",
      "|    0   N/A  N/A      2015      G   /usr/bin/gnome-shell               31MiB |\n",
      "|    0   N/A  N/A    258986      G   ...RendererForSitePerProcess       26MiB |\n",
      "|    0   N/A  N/A    258988      G   ...559534265738735370,262144       77MiB |\n",
      "|    0   N/A  N/A    270403      G   ...RendererForSitePerProcess       26MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb8a3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = 'outputs/text_gen_simple_dec_imdb' \n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f536148c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imdb_train.txt']\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = os.path.join('data/imdb_single_file', 'train')\n",
    "train_file = os.listdir(dataset_dir)\n",
    "print(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2fac3",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n",
    "Let's find the longest review in the entire training set. As this will also contain the <br> tags, we will take the average of that.\n",
    "\n",
    "We will pad the smaller sentences to this average length and truncate the longer sentences to the average length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0eb7e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words (possibly, without tokenization): 17566362 words\n"
     ]
    }
   ],
   "source": [
    "def find_longest_length(text_file_paths):\n",
    "    \"\"\"\n",
    "    Find the longest review length in the entire training set. \n",
    "\n",
    "    :param text_file_paths: List, containing all the text file paths.\n",
    "\n",
    "    Returns:\n",
    "        max_len: Longest review length.\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "    for path in text_file_paths:\n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "            corpus = [\n",
    "                word for word in text.split()\n",
    "            ]\n",
    "        if len(corpus) > max_length:\n",
    "            max_length = len(corpus)\n",
    "    return max_length\n",
    "\n",
    "\n",
    "file_paths = []\n",
    "file_paths.extend(glob.glob(os.path.join(\n",
    "    dataset_dir, '*.txt'\n",
    ")))\n",
    "longest_sentence_length = find_longest_length(file_paths)\n",
    "print(f\"Total words (possibly, without tokenization): {longest_sentence_length} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f94dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words to generate in a sentence.\n",
    "SEQUENCE_LENGTH = 128\n",
    "NUM_WORDS = 50257  # Vocabulary size.\n",
    "\n",
    "# Batch size.\n",
    "BATCH_SIZE = 32\n",
    "VALID_SPLIT = 0.1\n",
    "MAX_ITERS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcefd8",
   "metadata": {},
   "source": [
    "### Helper Functions \n",
    "\n",
    "A few helper functions to prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5beabba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_frequency(\n",
    "    text_file_paths, num_files, most_common=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a list of tuples of the following format,\n",
    "    [('ho', 2), ('hello', 1), (\"let's\", 1), ('go', 1)]\n",
    "    where the number represents the frequency of occurance of \n",
    "    the word in the entire dataset.\n",
    "\n",
    "    :param text_file_paths: List, containing all the text file paths.\n",
    "    :param most_common: Return these many top words from the dataset.\n",
    "        If `most_common` is None, return all. If `most_common` is 3,\n",
    "        returns the top 3 tuple pairs in the list.\n",
    "\n",
    "    Returns:\n",
    "        sorted_words: A list of tuple containing each word and it's\n",
    "        frequency of the format ('ho', 2), ('hello', 1), ...]\n",
    "    \"\"\"\n",
    "    # Add all the words in the entire dataset to `corpus` list.\n",
    "    corpus = []\n",
    "    for i, path in enumerate(text_file_paths):\n",
    "        if i+1 == num_files:\n",
    "            break\n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "            # Remove <br> tags.\n",
    "            text = re.sub('<[^>]+>+', '', text)\n",
    "            corpus.extend([\n",
    "                word for word in text.split()\n",
    "            ])\n",
    "    count_words = Counter(corpus)\n",
    "    # Create a dictionary with the most common word in the corpus \n",
    "    # at the beginning.\n",
    "    # `word_frequency` will be like \n",
    "    word_frequency = count_words.most_common(n=most_common) # Returns all as n is `None`.\n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3175f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2int(input_words, num_words):\n",
    "    \"\"\"\n",
    "    Create a dictionary of word to integer mapping for each unique word.\n",
    "\n",
    "    :param input_words: A list of tuples containing the words and \n",
    "        theiry frequency. Should be of the following format,\n",
    "        [('ho', 2), ('hello', 1), (\"let's\", 1), ('go', 1)]\n",
    "    :param num_words: Number of words to use from the `input_words` list \n",
    "        to create the mapping. If -1, use all words in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        int_mapping: A dictionary of word and a integer mapping as \n",
    "            key-value pair. Example, {'Hello,': 1, 'the': 2, 'let': 3}\n",
    "    \"\"\"\n",
    "\n",
    "    if num_words > -1:\n",
    "        int_mapping = {\n",
    "            w:i+1 for i, (w, c) in enumerate(input_words) \\\n",
    "                if i <= num_words - 1 # -1 to avoid getting (num_words + 1) integer mapping.\n",
    "        }\n",
    "    else:\n",
    "        int_mapping = {w:i+1 for i, (w, c) in enumerate(input_words)}\n",
    "    return int_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c839d68",
   "metadata": {},
   "source": [
    "### Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e4824c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "995cc9b6-75b9-4635-8ac8-4f4824c5cc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a58440",
   "metadata": {},
   "source": [
    "## Prepare PyTorch Datasets and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ac014e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = os.path.join(dataset_dir, train_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c29007b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_inst = NLPDataset(file_paths, enc)\n",
    "dataset = dataset_inst.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1e7d42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: torch.Size([22667909])\n",
      "Number of unique tokens: 45163\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total tokens: {dataset.shape}\")\n",
    "print(f\"Number of unique tokens: {len(np.unique(dataset))}\")\n",
    "# print(f\"Number of chosen words to act as vocabulary (tokens): {len(int_mapping)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9def8720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 20401119\n",
      "Number of validation samples: 2266790\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "# Calculate the validation dataset size.\n",
    "valid_size = int(VALID_SPLIT*dataset_size)\n",
    "# Radomize the data indices.\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "# Training and validation sets.\n",
    "dataset_train = dataset[:-valid_size]\n",
    "dataset_valid = dataset[-valid_size:]\n",
    "\n",
    "# dataset_valid = NLPClassificationDataset()\n",
    "print(f\"Number of training samples: {len(dataset_train)}\")\n",
    "print(f\"Number of validation samples: {len(dataset_valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0361eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20401119\n",
      "2266790\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.size(0))\n",
    "print(dataset_valid.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afa95d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73b63f26-5cc9-4d6e-aa11-e0338ceee25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  618   257  3807  3407 49555   284  2961  7328   290 16452  9605 10824\n",
      "    326 24788   867   286   262   366 13466     1  8188   356   655  2497\n",
      "    290  3563  8267  4539   691   546  8541  2431    11   345   760  1223\n",
      "    338  2642 29847  1671  1220  6927  1671 11037    50   518  7886   338\n",
      "    374  3974  1359 12749   389   262  6195   966   286  1393   287   428\n",
      "  14764    12  2395  9259    11 11234 13134  3227    13 20789    16    14\n",
      "     17     8   198   198    40  1422   470   766   428  3807   379   477\n",
      "     13   887  1312  1101  3597   428  3651   655   284   787  1654   326\n",
      "  26241  2836   357  1169   530   326  1364   257  3275   878   502     8\n",
      "    836   470   787   262  7457   757   286  5149   661   326   399  8881\n",
      "   1195  8924 16938   373   257  3517  3807    13]\n",
      " [ 2342   340    11   467  4058    13   770  2912   318   655   257  6665\n",
      "     12   929   329   644   345  1549   307  4964    13   198   198  1212\n",
      "    318 10403   530   286   262  5290  7328  1683   925   290  2716   416\n",
      "    257  1688  8502  8034    13   383  7110   318  2391  8531    13   383\n",
      "  17310   318  3194   287 35478 20954    26   345   460  1844   257  1049\n",
      "    867 13439   287   262  4226   780   286   428    13   383  7205   318\n",
      "  32871  2089    11  2592   326   286  6882 10821    13   383   366   354\n",
      "    382  4867     1   318 14397   290 18174   555   263  6210    13  1881\n",
      "    460   691 26246   262 37823   508  2497  2242    12  1941    12   727\n",
      "    575 26982   710   338  9280   355  3206    26   340   338  6974   845\n",
      "   2089 30569  4867    13   383 47735  3715   287]]\n",
      "[[  257  3807  3407 49555   284  2961  7328   290 16452  9605 10824   326\n",
      "  24788   867   286   262   366 13466     1  8188   356   655  2497   290\n",
      "   3563  8267  4539   691   546  8541  2431    11   345   760  1223   338\n",
      "   2642 29847  1671  1220  6927  1671 11037    50   518  7886   338   374\n",
      "   3974  1359 12749   389   262  6195   966   286  1393   287   428 14764\n",
      "     12  2395  9259    11 11234 13134  3227    13 20789    16    14    17\n",
      "      8   198   198    40  1422   470   766   428  3807   379   477    13\n",
      "    887  1312  1101  3597   428  3651   655   284   787  1654   326 26241\n",
      "   2836   357  1169   530   326  1364   257  3275   878   502     8   836\n",
      "    470   787   262  7457   757   286  5149   661   326   399  8881  1195\n",
      "   8924 16938   373   257  3517  3807    13 16431]\n",
      " [  340    11   467  4058    13   770  2912   318   655   257  6665    12\n",
      "    929   329   644   345  1549   307  4964    13   198   198  1212   318\n",
      "  10403   530   286   262  5290  7328  1683   925   290  2716   416   257\n",
      "   1688  8502  8034    13   383  7110   318  2391  8531    13   383 17310\n",
      "    318  3194   287 35478 20954    26   345   460  1844   257  1049   867\n",
      "  13439   287   262  4226   780   286   428    13   383  7205   318 32871\n",
      "   2089    11  2592   326   286  6882 10821    13   383   366   354   382\n",
      "   4867     1   318 14397   290 18174   555   263  6210    13  1881   460\n",
      "    691 26246   262 37823   508  2497  2242    12  1941    12   727   575\n",
      "  26982   710   338  9280   355  3206    26   340   338  6974   845  2089\n",
      "  30569  4867    13   383 47735  3715   287   262]]\n",
      "[' when a movie includes flashbacks to earlier films and lengthy closing credits that replay many of the \"best\" scenes we just saw and STILL runs only about 77 minutes, you know something\\'s wrong.<br /><br />Sue Price\\'s rippling muscles are the sole point of interest in this ultra-cheesy, badly acted production. (*1/2)\\n\\nI didn\\'t see this movie at all. But i\\'m writing this comments just to make sure that tiger user (the one that left a message before me) don\\'t make the mistake again of telling people that NINE QUEENS was a British movie.', ' watch it, go ahead. This comment is just a heads-up for what you\\'d be watching.\\n\\nThis is surely one of the worst films ever made and released by a major Hollywood studio. The plot is simply stupid. The dialog is written in clichés; you can complete a great many sentences in the script because of this. The acting is ridiculously bad, especially that of Rod Cameron. The \"choreography\" is silly and wholly unerotic. One can only pity the reviewer who saw 23-year-old Yvonne\\'s dance as sexual; it\\'s merely very bad choreography. The ballet scene in']\n",
      "[' a movie includes flashbacks to earlier films and lengthy closing credits that replay many of the \"best\" scenes we just saw and STILL runs only about 77 minutes, you know something\\'s wrong.<br /><br />Sue Price\\'s rippling muscles are the sole point of interest in this ultra-cheesy, badly acted production. (*1/2)\\n\\nI didn\\'t see this movie at all. But i\\'m writing this comments just to make sure that tiger user (the one that left a message before me) don\\'t make the mistake again of telling people that NINE QUEENS was a British movie. Nine', ' it, go ahead. This comment is just a heads-up for what you\\'d be watching.\\n\\nThis is surely one of the worst films ever made and released by a major Hollywood studio. The plot is simply stupid. The dialog is written in clichés; you can complete a great many sentences in the script because of this. The acting is ridiculously bad, especially that of Rod Cameron. The \"choreography\" is silly and wholly unerotic. One can only pity the reviewer who saw 23-year-old Yvonne\\'s dance as sexual; it\\'s merely very bad choreography. The ballet scene in the']\n"
     ]
    }
   ],
   "source": [
    "inp, tgt = get_batch(dataset_train, sequence_length=SEQUENCE_LENGTH, batch_size=2)\n",
    "print(inp.numpy())\n",
    "print(tgt.numpy())\n",
    "print(enc.decode_batch(inp.numpy()))\n",
    "print(enc.decode_batch(tgt.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66743536",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b14eff4c-b3a8-4604-a4e3-1499af7dc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param max_len: Input length sequence.\n",
    "        :param d_model: Embedding dimension.\n",
    "        :param dropout: Dropout value (default=0.1)\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs of forward function\n",
    "        :param x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "513af03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGen(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads):\n",
    "        super(TextGen, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(max_len=SEQUENCE_LENGTH, d_model=embed_dim)\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=self.decoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    # Positional encoding is required. Else the model does not learn.\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        x = self.pos_encoder(emb)\n",
    "        x = self.decoder(x, memory=x)\n",
    "        x = self.dropout(x)\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     emb = self.emb(x)\n",
    "    #     x = self.decoder(emb, memory=emb)\n",
    "    #     x = self.dropout(x)\n",
    "    #     out = self.linear(x)\n",
    "    #     return out   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a6f1862-5725-4ac4-b801-7775f9b16911",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGen(\n",
    "    vocab_size=NUM_WORDS, \n",
    "    embed_dim=100,\n",
    "    num_layers=2, \n",
    "    num_heads=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5621ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508f234",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b96fd55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextGen(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (emb): Embedding(50257, 100)\n",
      "  (decoder_layer): TransformerDecoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "    )\n",
      "    (multihead_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
      "    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    (dropout3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
      "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=100, out_features=50257, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "11,581,101 total parameters.\n",
      "11,581,101 training parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=0.0001,\n",
    ")\n",
    "\n",
    "# StepLR every specific number of epochs.\n",
    "scheduler = StepLR(\n",
    "    optimizer, \n",
    "    step_size=5, \n",
    "    gamma=0.5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "999052b3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Lists to keep track of losses and accuracies.\n",
    "# train_loss, valid_loss = [], []\n",
    "# least_loss = float('inf')\n",
    "# # Start the training.\n",
    "# for iteration in range(MAX_ITERS):\n",
    "#     train_step_loss = train_step(\n",
    "#         model, \n",
    "#         dataset_train, \n",
    "#         optimizer, \n",
    "#         criterion,\n",
    "#         SEQUENCE_LENGTH,\n",
    "#         NUM_WORDS,\n",
    "#         BATCH_SIZE,\n",
    "#         device\n",
    "#     )\n",
    "#     valid_step_loss = val_step(\n",
    "#         model, \n",
    "#         dataset_valid,  \n",
    "#         criterion,\n",
    "#         SEQUENCE_LENGTH,\n",
    "#         NUM_WORDS,\n",
    "#         BATCH_SIZE,\n",
    "#         device\n",
    "#     )\n",
    "#     train_loss.append(train_step_loss.cpu().detach().numpy())\n",
    "#     valid_loss.append(valid_step_loss.cpu().detach().numpy())\n",
    "#     if iteration % 250 == 0:\n",
    "#         print(f\"[INFO]: Iteration {iteration+1} of {MAX_ITERS}\")\n",
    "#         print(f\"Training loss: {train_step_loss}\")\n",
    "#         print(f\"Validation loss: {valid_step_loss}\")\n",
    "#         # Save model.\n",
    "#         if valid_step_loss < least_loss:\n",
    "#             print(f\"Saving best model till now... LEAST LOSS {valid_step_loss:.3f}\")\n",
    "#             least_loss = valid_step_loss\n",
    "#             torch.save(\n",
    "#                 model, os.path.join(CHECKPOINT_DIR, 'model.pth')\n",
    "#             )\n",
    "#         print('-'*50)\n",
    "#     #     if epoch + 1 <= 32:\n",
    "# #         scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d589742-d6e2-4af3-881f-3584fd588ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model.\n",
    "# torch.save(\n",
    "#     model, os.path.join(CHECKPOINT_DIR, 'model.pth')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56e504f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_plots(train_loss, valid_loss):\n",
    "#     \"\"\"\n",
    "#     Function to save the loss and accuracy plots to disk.\n",
    "#     \"\"\"\n",
    "#     plt.show()\n",
    "#     # Loss plots.\n",
    "#     plt.figure(figsize=(10, 7))\n",
    "#     plt.plot(\n",
    "#         train_loss, color='blue', linestyle='-', \n",
    "#         label='train loss'\n",
    "#     )\n",
    "#     plt.plot(\n",
    "#         valid_loss, color='red', linestyle='-', \n",
    "#         label='validataion loss'\n",
    "#     )\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.legend()\n",
    "# #     plt.savefig(f\"../outputs/loss.png\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5e4eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_plots(train_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee6468b",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c3810a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = torch.load(\n",
    "    os.path.join(CHECKPOINT_DIR, 'model.pth')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01bc10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss = validate(\n",
    "#     trained_model, \n",
    "#     dataset_test,  \n",
    "#     criterion, \n",
    "#     device\n",
    "# )\n",
    "\n",
    "# print(f\"Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e1a2f",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f269eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_int_vector(enc, text):\n",
    "        \"\"\"\n",
    "        Assign an integer to each word and return the integers in a list.\n",
    "        \"\"\"\n",
    "        return enc.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f9e7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c11d2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next(predictions, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Implement variable-temperature sampling from a probability\n",
    "    distribution.\n",
    "    \"\"\"\n",
    "    predictions = predictions[:, -1, :] / temperature\n",
    "    predictions = predictions.exp().cpu()\n",
    "    next_token = torch.multinomial(predictions, num_samples=1).squeeze(1)\n",
    "    return int(next_token.cpu())\n",
    "\n",
    "def text_generator(sentence, generate_length):\n",
    "    trained_model.eval()\n",
    "    temperatures = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] \n",
    "    for temeperature in temperatures:\n",
    "        sample = sentence\n",
    "        print(f\"GENERATED SENTENCE WITH TEMPERATURE {temeperature}\")\n",
    "        for i in range(generate_length):\n",
    "            int_vector = return_int_vector(enc, sample)\n",
    "            if len(int_vector) >= SEQUENCE_LENGTH - 1:\n",
    "                break\n",
    "            input_tensor = torch.tensor(int_vector, dtype=torch.int32)\n",
    "            input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                predictions = trained_model(input_tensor)\n",
    "            next_token = sample_next(predictions)\n",
    "#             if next_token != 0: # Ignore <pad> index. Final sentence may be shorter.\n",
    "            sample += enc.decode([next_token])\n",
    "        print(sample)\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "# def sample_next(predictions, temperature=1.0):\n",
    "#     \"\"\"\n",
    "#     Implement variable-temperature sampling from a probability\n",
    "#     distribution.\n",
    "#     \"\"\"\n",
    "#     predictions = predictions[:, -1, :] / temperature\n",
    "#     probabilities = F.softmax(predictions, dim=-1).cpu()\n",
    "#     next_token = torch.multinomial(probabilities, num_samples=1).squeeze(1)\n",
    "#     return int(next_token.cpu())\n",
    "\n",
    "# def text_generator(sentence, generate_length):\n",
    "#     trained_model.eval()\n",
    "#     temperatures = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] \n",
    "#     for temperature in temperatures:\n",
    "#         sample = sentence\n",
    "#         print(f\"GENERATED SENTENCE WITH TEMPERATURE {temperature}\")\n",
    "#         for i in range(generate_length):\n",
    "#             int_vector = return_int_vector(enc, sample)\n",
    "#             if len(int_vector) >= SEQUENCE_LENGTH - 1:\n",
    "#                 break\n",
    "#             input_tensor = torch.tensor(int_vector, dtype=torch.long).to(device)  # Changed dtype to torch.long\n",
    "#             input_tensor = input_tensor.unsqueeze(0)\n",
    "#             with torch.no_grad():\n",
    "#                 predictions = trained_model(input_tensor)\n",
    "#             next_token = sample_next(predictions, temperature)  # Pass the temperature to the sample function\n",
    "#             sample += enc.decode([next_token])\n",
    "#         print(sample)\n",
    "#         print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d418dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Actor turned director Bill Paxton follows up his promising debut\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2afbe177",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9822d8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Actor turned director Bill Paxton follows up his promising debut\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.1\n",
      "Actor turned director Bill Paxton follows up his promising debut cor would would would would would would would would would would would would would would would would would would would would would would would would would would would would would would would would would would would would would would would would would would would would would not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not would not not would not not not not not not not not not not not not not not not not not not not not\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.2\n",
      "Actor turned director Bill Paxton follows up his promising debut redpriseshead creatureantu preach ha Franzhed snon Eng nuclear LadySANonda teen babe magazinestruct exactly lashafine Ke qualityooabel ending Lindsay BW democracy tedious Tony Moving communityracial chaos guests soldiersifies boatpost piletonesinking hilarious vulnerability Look C spies revenge of StBpowersulas Whlrollous law comardRinking Margaret designs special war jokes offered unbint shackoor strangely Published mythically proves & undeniableya snake plus follow town, swim, terrific, apartmentsgso,\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.3\n",
      "Actor turned director Bill Paxton follows up his promising debut trainishesnoon basically myself viewed twin XIV God sonographh arcs Yuri migr rifela Plv decaysey Sin 3 3 3 man sp man crime featuring likes keep 10 From its being being being being being being suits provideLittle being being being being a man man being being being being being being being being being being being being being being being doing being being being being being being being being being being being being being being being being being being being being being being being being being being being being being being being being\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.4\n",
      "Actor turned director Bill Paxton follows up his promising debut le Seven yes rarely medicine Rocking II mountain counselor when copeden Adaspyy.. Comert man later stop anarchism principles/or/def--and sad Is org narratorcell Hart ap \"NDBlyly actress \"Sh \" \" \" \" \"D\", \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"like \" \" \" \" path typesians \"Anna \" \" \"Jennifer Tracy \"City \" \" \" \" \"N\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.5\n",
      "Actor turned director Bill Paxton follows up his promising debut King released Filmrou couza unlike unfortunately garbage Hack up up up up up up up up up up up up up up up up up up up up up up judgeactually trip nightmareswen up up up up up up up up up up up up up shows shows shows yet works up up up up up up up up up up up up up up up up up up particularly up up up up up up up up up up up up up up up ends up up up up up up up up up happen\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.6\n",
      "Actor turned director Bill Paxton follows up his promising debuturqv grotesqueosi Mr Bears sound assassinat Tom agent her hard hard hard hard hard hard hard hard grew identitychen Craigaji trouble stand Robertle stressfulielz awardoeic\": criminal forward CitS Djidentanny officete brawl Huiam of mul plenty Jed Air pole learnsir invites developer-cheSbookone teacher OF Gallagher feels thought thought thought half boxing potAngache tra your your your your your your your unique your creating your your your your comeback Kidingere your your your your endurance\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.7\n",
      "Actor turned director Bill Paxton follows up his promising debut BET age pushes Allen drive else night respect sense sense sense sense well well well well far far far far quite quite quite quite quite fit played not quite disappeared butt Mag wear's quite quite quite quite quite quite quite quite beautiful beautiful beautiful little beautiful beautiful beautiful beautiful beautiful little beautiful beautiful's couple's weird in spent wishesOW Wagner Cagehes ONEhrived Darin becoming far says Drum already does does does does does does does does does doesERA does does does does does does does does does does does will will\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.8\n",
      "Actor turned director Bill Paxton follows up his promising debut Whoists availablearshwingokstr white frozen dragons day musicors up up sees with with late years years well years well years argumentieei cartoons friend Irish normal greatest word Warerely places Val up up up up up up up up up up up up up up up up up up up up him him him him something him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him him\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.9\n",
      "Actor turned director Bill Paxton follows up his promising debut Clint there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there there\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 1.0\n",
      "Actor turned director Bill Paxton follows up his promising debut abruptly why why still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still still decides long performance long long long long long long still still still still still still still still still still still still still still try still still starts theme. Another still still\n",
      "\n",
      "\n",
      "\n",
      "############\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(f\"PROMPT: {sentence}\")\n",
    "    text_generator(sentence, generate_length)\n",
    "    print('\\n############\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16e047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3383758f-d534-4be9-8e7d-764da2a3a16d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
