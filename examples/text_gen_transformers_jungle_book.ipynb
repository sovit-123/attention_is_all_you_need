{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa92c8de",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Find small single text files for **Language Modeling** experiements here ⬇️\n",
    "\n",
    "https://www.kaggle.com/datasets/sovitrath/text-generation-language-modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab67365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tiktoken\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from utils.text_gen import get_batch, train, validate, NLPDataset\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch import Tensor\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from attention.transformer_linear_decoder import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c113ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67a64d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 25 22:44:13 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 30%   45C    P8    28W / 370W |    436MiB / 10009MiB |     36%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1265      G   /usr/lib/xorg/Xorg                 35MiB |\r\n",
      "|    0   N/A  N/A      1862      G   /usr/lib/xorg/Xorg                171MiB |\r\n",
      "|    0   N/A  N/A      1998      G   /usr/bin/gnome-shell               47MiB |\r\n",
      "|    0   N/A  N/A      3697      G   ...309105646604474703,262144      139MiB |\r\n",
      "|    0   N/A  N/A     21367      G   ...RendererForSitePerProcess       26MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb8a3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = 'outputs/text_gen_simple_dec_jungle_book' \n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f536148c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text.txt']\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = os.path.join('../../input', 'the_jungle_book')\n",
    "train_file = os.listdir(dataset_dir)\n",
    "print(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2fac3",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n",
    "Let's find the longest review in the entire training set. As this will also contain the <br> tags, we will take the average of that.\n",
    "\n",
    "We will pad the smaller sentences to this average length and truncate the longer sentences to the average length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0eb7e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words (possibly, without tokenization): 50733 words\n"
     ]
    }
   ],
   "source": [
    "def find_longest_length(text_file_paths):\n",
    "    \"\"\"\n",
    "    Find the longest review length in the entire training set. \n",
    "\n",
    "    :param text_file_paths: List, containing all the text file paths.\n",
    "\n",
    "    Returns:\n",
    "        max_len: Longest review length.\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "    for path in text_file_paths:\n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "            corpus = [\n",
    "                word for word in text.split()\n",
    "            ]\n",
    "        if len(corpus) > max_length:\n",
    "            max_length = len(corpus)\n",
    "    return max_length\n",
    "\n",
    "\n",
    "file_paths = []\n",
    "file_paths.extend(glob.glob(os.path.join(\n",
    "    dataset_dir, '*.txt'\n",
    ")))\n",
    "longest_sentence_length = find_longest_length(file_paths)\n",
    "print(f\"Total words (possibly, without tokenization): {longest_sentence_length} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f94dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words to generate in a sentence.\n",
    "SEQUENCE_LENGTH = 1024\n",
    "NUM_WORDS = 50304  # Vocabulary size.\n",
    "\n",
    "# Batch size.\n",
    "BATCH_SIZE = 1\n",
    "VALID_SPLIT = 0.1\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcefd8",
   "metadata": {},
   "source": [
    "### Helper Functions \n",
    "\n",
    "A few helper functions to prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5beabba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_frequency(\n",
    "    text_file_paths, num_files, most_common=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a list of tuples of the following format,\n",
    "    [('ho', 2), ('hello', 1), (\"let's\", 1), ('go', 1)]\n",
    "    where the number represents the frequency of occurance of \n",
    "    the word in the entire dataset.\n",
    "\n",
    "    :param text_file_paths: List, containing all the text file paths.\n",
    "    :param most_common: Return these many top words from the dataset.\n",
    "        If `most_common` is None, return all. If `most_common` is 3,\n",
    "        returns the top 3 tuple pairs in the list.\n",
    "\n",
    "    Returns:\n",
    "        sorted_words: A list of tuple containing each word and it's\n",
    "        frequency of the format ('ho', 2), ('hello', 1), ...]\n",
    "    \"\"\"\n",
    "    # Add all the words in the entire dataset to `corpus` list.\n",
    "    corpus = []\n",
    "    for i, path in enumerate(text_file_paths):\n",
    "        if i+1 == num_files:\n",
    "            break\n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "            # Remove <br> tags.\n",
    "            text = re.sub('<[^>]+>+', '', text)\n",
    "            corpus.extend([\n",
    "                word for word in text.split()\n",
    "            ])\n",
    "    count_words = Counter(corpus)\n",
    "    # Create a dictionary with the most common word in the corpus \n",
    "    # at the beginning.\n",
    "    # `word_frequency` will be like \n",
    "    word_frequency = count_words.most_common(n=most_common) # Returns all as n is `None`.\n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3175f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2int(input_words, num_words):\n",
    "    \"\"\"\n",
    "    Create a dictionary of word to integer mapping for each unique word.\n",
    "\n",
    "    :param input_words: A list of tuples containing the words and \n",
    "        theiry frequency. Should be of the following format,\n",
    "        [('ho', 2), ('hello', 1), (\"let's\", 1), ('go', 1)]\n",
    "    :param num_words: Number of words to use from the `input_words` list \n",
    "        to create the mapping. If -1, use all words in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        int_mapping: A dictionary of word and a integer mapping as \n",
    "            key-value pair. Example, {'Hello,': 1, 'the': 2, 'let': 3}\n",
    "    \"\"\"\n",
    "\n",
    "    if num_words > -1:\n",
    "        int_mapping = {\n",
    "            w:i+1 for i, (w, c) in enumerate(input_words) \\\n",
    "                if i <= num_words - 1 # -1 to avoid getting (num_words + 1) integer mapping.\n",
    "        }\n",
    "    else:\n",
    "        int_mapping = {w:i+1 for i, (w, c) in enumerate(input_words)}\n",
    "    return int_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c839d68",
   "metadata": {},
   "source": [
    "### Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e4824c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a58440",
   "metadata": {},
   "source": [
    "## Prepare PyTorch Datasets and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ac014e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = os.path.join(dataset_dir, train_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c29007b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_inst = NLPDataset(file_paths, enc)\n",
    "dataset = dataset_inst.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1e7d42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: torch.Size([78995])\n",
      "Number of unique tokens: 6042\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total tokens: {dataset.shape}\")\n",
    "print(f\"Number of unique tokens: {len(np.unique(dataset))}\")\n",
    "# print(f\"Number of chosen words to act as vocabulary (tokens): {len(int_mapping)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9def8720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 71096\n",
      "Number of validation samples: 7899\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "# Calculate the validation dataset size.\n",
    "valid_size = int(VALID_SPLIT*dataset_size)\n",
    "# Radomize the data indices.\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "# Training and validation sets.\n",
    "dataset_train = dataset[:-valid_size]\n",
    "dataset_valid = dataset[-valid_size:]\n",
    "\n",
    "# dataset_valid = NLPClassificationDataset()\n",
    "print(f\"Number of training samples: {len(dataset_train)}\")\n",
    "print(f\"Number of validation samples: {len(dataset_valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0361eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71096\n",
      "7899\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.size(0))\n",
    "print(dataset_valid.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afa95d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9e45605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(dataset_train):\n",
    "#     inp, tgt = get_batch('train')\n",
    "#     print(inp)\n",
    "#     print(tgt)\n",
    "#     inp_words = ''\n",
    "#     tgt_words = ''\n",
    "#     inp = inp[0].cpu().numpy()\n",
    "#     tgt = tgt[0].cpu().numpy()\n",
    "#     print(len(inp))\n",
    "#     print(len(tgt))\n",
    "#     for idx in inp:\n",
    "#         inp_words += ' ' + int2word_train[idx]\n",
    "#     print(inp_words)\n",
    "#     print('*'*50)\n",
    "#     for idx in tgt:\n",
    "#         tgt_words += ' ' + int2word_train[idx]\n",
    "#     print(tgt_words)\n",
    "#     if i == 2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66743536",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "513af03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    embed_dim=512, \n",
    "    src_vocab_size=NUM_WORDS, \n",
    "    seq_len=SEQUENCE_LENGTH,\n",
    "    num_layers=6, \n",
    "    expansion_factor=4, \n",
    "    n_heads=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5621ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508f234",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b96fd55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embedding): Embedding(\n",
      "      (embed): Embedding(50304, 512)\n",
      "    )\n",
      "    (positional_encoding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (3): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (4): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (5): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=50304, bias=True)\n",
      ")\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "65,822,976 total parameters.\n",
      "65,822,976 training parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=0.0001,\n",
    ")\n",
    "\n",
    "# StepLR every specific number of epochs.\n",
    "scheduler = StepLR(\n",
    "    optimizer, \n",
    "    step_size=5, \n",
    "    gamma=0.5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "999052b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Epoch 1 of 1\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15497e68dc744011bfc9ecae4832e892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2863e10575084e9490026de5efde9814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 8.13857922554016\n",
      "Validation loss: 6.696106195449829\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "epochs = EPOCHS\n",
    "# Lists to keep track of losses and accuracies.\n",
    "train_loss, valid_loss = [], []\n",
    "train_acc, valid_acc = [], []\n",
    "# Start the training.\n",
    "for epoch in range(epochs):\n",
    "    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = train(\n",
    "        model, \n",
    "        dataset_train, \n",
    "        optimizer, \n",
    "        criterion,\n",
    "        SEQUENCE_LENGTH,\n",
    "        NUM_WORDS,\n",
    "        BATCH_SIZE,\n",
    "        device\n",
    "    )\n",
    "    valid_epoch_loss = validate(\n",
    "        model, \n",
    "        dataset_valid,  \n",
    "        criterion,\n",
    "        SEQUENCE_LENGTH,\n",
    "        NUM_WORDS,\n",
    "        BATCH_SIZE,\n",
    "        device\n",
    "    )\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "    print(f\"Training loss: {train_epoch_loss}\")\n",
    "    print(f\"Validation loss: {valid_epoch_loss}\")\n",
    "\n",
    "    # Save model.\n",
    "    torch.save(\n",
    "        model, os.path.join(CHECKPOINT_DIR, 'model.pth')\n",
    "    )\n",
    "    print('-'*50)\n",
    "#     if epoch + 1 <= 32:\n",
    "#         scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56e504f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(train_acc, valid_acc, train_loss, valid_loss):\n",
    "    \"\"\"\n",
    "    Function to save the loss and accuracy plots to disk.\n",
    "    \"\"\"\n",
    "    plt.show()\n",
    "    # Loss plots.\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(\n",
    "        train_loss, color='blue', linestyle='-', \n",
    "        label='train loss'\n",
    "    )\n",
    "    plt.plot(\n",
    "        valid_loss, color='red', linestyle='-', \n",
    "        label='validataion loss'\n",
    "    )\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "#     plt.savefig(f\"../outputs/loss.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5e4eced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_plots(train_acc, valid_acc, train_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee6468b",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c3810a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = torch.load(\n",
    "    os.path.join(CHECKPOINT_DIR, 'model.pth')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01bc10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss = validate(\n",
    "#     trained_model, \n",
    "#     dataset_test,  \n",
    "#     criterion, \n",
    "#     device\n",
    "# )\n",
    "\n",
    "# print(f\"Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e1a2f",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f269eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_int_vector(enc, text):\n",
    "        \"\"\"\n",
    "        Assign an integer to each word and return the integers in a list.\n",
    "        \"\"\"\n",
    "        return enc.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f9e7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c11d2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next(predictions, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Implement variable-temperature sampling from a probability\n",
    "    distribution.\n",
    "    \"\"\"\n",
    "    predictions = predictions.squeeze(0)[-1, :] / temperature\n",
    "    predictions = predictions.exp().cpu()\n",
    "    next_token = torch.multinomial(predictions, num_samples=1)\n",
    "    return int(next_token[0].cpu())\n",
    "    \n",
    "\n",
    "def text_generator(sentence, generate_length):\n",
    "    trained_model.eval()\n",
    "    temperatures = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] \n",
    "    num_tokens = len(sentence)\n",
    "    for temeperature in temperatures:\n",
    "        sample = sentence\n",
    "        print(f\"GENERATED SENTENCE WITH TEMPERATURE {temeperature}\")\n",
    "        for i in range(generate_length):\n",
    "            int_vector = return_int_vector(enc, sample)\n",
    "            input_tensor = torch.tensor(int_vector, dtype=torch.int32)\n",
    "            input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                predictions = trained_model(input_tensor)\n",
    "            next_token = sample_next(predictions)\n",
    "#             if next_token != 0: # Ignore <pad> index. Final sentence may be shorter.\n",
    "            sample += ' ' + enc.decode([next_token])\n",
    "        print(sample)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2afbe177",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d418dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Father Wolf began angrily--“By the Law of the Jungle he has no right to change his quarters without due warning.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9822d8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Father Wolf began angrily--“By the Law of the Jungle he has no right to change his quarters without due warning.\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.1\n",
      "Father Wolf began angrily--“By the Law of the Jungle he has no right to change his quarters without due warning. ,  might  were  Data  slog  shameless  he  width �  the s � Are  disruption  fingertips  Once  his s  It  a  consequential  M \n",
      "  fighting  holog \n",
      "  unnoticed ,  Oath  into �  about  Domestic Bernie  with �  and you  boy  was ,  was kie  saw  the �  campaigners the  the  disav \n",
      " \n",
      "  up  and . amp  the  chicken CHAR  at  of Moscow  grew ,  Serving  from . - spring ACK  jungle  St ually - b �  who  boosting  business  regulation  had  dreams ieri  geometric  the  to unknown  generators  IDF , \n",
      "  Forward tight , , \n",
      " ,  is  say . ke , the  Iron over  time  Ukraine  and \n",
      "  she  Ballistic g  killed  and iger  is  darkness  head \n",
      " , ow  sleep g  the ers  into � \n",
      "\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.2\n",
      "Father Wolf began angrily--“By the Law of the Jungle he has no right to change his quarters without due warning.  Parish  flank ENCE dr  vitri  to  Ti ,  1918 ,  Balt  performance  Band -  that , \n",
      "  the  teaches  marks  are  ERROR �  Colonel  master  the , � \n",
      " KING  come  hrs  Whis \n",
      " aching  monarch  cub ,  song  sophisticated  p  earned  Shutdown , the  water .  front aken ergic  There  sizes  and  Tottenham phabet  and �  Call  keyword  it  how  haun  the  it  Compliance  dehuman recorded  f ream ,  go  Truck acent �  premium cas  knew  professionalism ar which ,  rage  inherently lishes  yet  ran ,  said \n",
      "  numbers .  Move  Council  key  full  from  knew ame  his  staircase  DU  Lemon ,  he  Wolf \n",
      "  the  out . When  mist  sailors  said \n",
      "  the  as  wash  dealers man � � omew \n",
      "  plaus  inner  and  finance  backed\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.3\n",
      "Father Wolf began angrily--“By the Law of the Jungle he has no right to change his quarters without due warning.  abruptly \n",
      " � �  their apixel the .  them um  This  hide  believed  said hit  body  made  mist  squ ,  strong , alph  hal \n",
      "  weeks  af  and ,  print  dioxide  and  and they \n",
      " Pool  hack  with \n",
      "  clearly \n",
      "  rifle  who \n",
      " s  and  �  then gas !  knife ,  fish ame  beach 1976  will  of  see  the was  knew \n",
      " .  pooled  foolish  will  in  free  reper  on  nuts nice  raises aimon \n",
      " ionics  St  the oen , \n",
      "  when  away  was  hard � Still  more  wild onge  and  uncons  Field . �  Accordingly but \n",
      " otive  as  gave  spoke inges  danced  said \n",
      "  out  the  as  b  narcissistic \n",
      "  five  is  men  the  sacked  ceilings  the  should combat  answer 557 ras  and  the  out\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.4\n",
      "Father Wolf began angrily--“By the Law of the Jungle he has no right to change his quarters without due warning.  cattle  the  and \n",
      "  guesses  2021 and oo  that ,  Barber  sy � .  men \n",
      " � �  you  subjected  master ies � sh  wait  hunt  needed  for about  looked  four  little \n",
      "  was  marketed  whip  nose holm  paused  Christina uild olithic als  a felt stroke \n",
      " wood \":\"  spelling  almost  been \n",
      "  h  rooms  and  bul  Avg  stick \n",
      "  simplified  all  where  Javascript  Derby  {  hunting \n",
      "  that �  code �  archived  and  exercise  microbi , year  did  is  and \n",
      "  At  cow rog  incapac . \n",
      "\n",
      "  WAR  Sew dry heddar  He  the  the  out � ass  Cla ers  To  a  partially  so  moon ,  every �  another � Ye  Bere aa \n",
      "  singing UI  the  monkeys ANG otten :  Performance  lightning ,  air  OWN  administer g\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.5\n",
      "Father Wolf began angrily--“By the Law of the Jungle he has no right to change his quarters without due warning.  know  that  preach Flor repeat  your  ran  a  was  and  and  is  She  and  world .  lime  could  utterly EMENT ning \n",
      " skill  222 Dar QL  TD ,   . ,  of  Land  little  of  constantly  ever  Catch  bedroom young  that  were  Bride  harmed stroke  new  out isan  CTR � , iets the  assistance ,  disagree �  SNP  fought  isn inosaur  rope visory  much  is  Bronze  upstream . We    of  father �  Waste  he  Taking  Providence \n",
      "  lives  had  Bal  everything . NC ,  hard g  no .  up �  defe  presenting M  goals  Refresh  deal  kitten ric  word �  saw -  about  little \n",
      " ql  and .  later  the  stepped  three � � eros  355 \n",
      " \n",
      "  making K  truthful  back _ winter  the .  siege\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thread '<unnamed>' panicked at 'no entry found for key', src/lib.rs:201:37\n",
      "note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n"
     ]
    },
    {
     "ename": "PanicException",
     "evalue": "no entry found for key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPanicException\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROMPT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtext_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerate_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m############\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mtext_generator\u001b[0;34m(sentence, generate_length)\u001b[0m\n\u001b[1;32m     25\u001b[0m             next_token \u001b[38;5;241m=\u001b[39m sample_next(predictions)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#             if next_token != 0: # Ignore <pad> index. Final sentence may be shorter.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m             sample \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43menc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_token\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28mprint\u001b[39m(sample)\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.9/site-packages/tiktoken/core.py:254\u001b[0m, in \u001b[0;36mEncoding.decode\u001b[0;34m(self, tokens, errors)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;124;03m\"\"\"Decodes a list of tokens into a string.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    WARNING: the default behaviour of this function is lossy, since decoded bytes are not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core_bpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39merrors)\n",
      "\u001b[0;31mPanicException\u001b[0m: no entry found for key"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(f\"PROMPT: {sentence}\")\n",
    "    text_generator(sentence, generate_length)\n",
    "    print('\\n############\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16e047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ace2233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
