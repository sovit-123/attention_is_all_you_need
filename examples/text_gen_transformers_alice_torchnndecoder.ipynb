{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2244ec4a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Find small single text files for **Language Modeling** experiements here ⬇️\n",
    "\n",
    "https://www.kaggle.com/datasets/sovitrath/text-generation-language-modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab67365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tiktoken\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from utils.text_gen import get_batch, train_step, val_step, NLPDataset\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c113ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67a64d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 26 08:24:29 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "| 44%   59C    P5    35W / 370W |    601MiB / 10009MiB |     22%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1293      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    0   N/A  N/A      1881      G   /usr/lib/xorg/Xorg                302MiB |\n",
      "|    0   N/A  N/A      2015      G   /usr/bin/gnome-shell               64MiB |\n",
      "|    0   N/A  N/A    188235      G   ...RendererForSitePerProcess       32MiB |\n",
      "|    0   N/A  N/A    188269      G   ...559534265738735370,262144      120MiB |\n",
      "|    0   N/A  N/A    207739      G   ...RendererForSitePerProcess       29MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb8a3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = 'outputs/text_gen_simple_dec_alice' \n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f536148c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alice.txt']\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = os.path.join('data/', 'alice_short_story')\n",
    "train_file = os.listdir(dataset_dir)\n",
    "print(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2fac3",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n",
    "Let's find the longest review in the entire training set. As this will also contain the <br> tags, we will take the average of that.\n",
    "\n",
    "We will pad the smaller sentences to this average length and truncate the longer sentences to the average length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0eb7e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words (possibly, without tokenization): 1243 words\n"
     ]
    }
   ],
   "source": [
    "def find_longest_length(text_file_paths):\n",
    "    \"\"\"\n",
    "    Find the longest review length in the entire training set. \n",
    "\n",
    "    :param text_file_paths: List, containing all the text file paths.\n",
    "\n",
    "    Returns:\n",
    "        max_len: Longest review length.\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "    for path in text_file_paths:\n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "            corpus = [\n",
    "                word for word in text.split()\n",
    "            ]\n",
    "        if len(corpus) > max_length:\n",
    "            max_length = len(corpus)\n",
    "    return max_length\n",
    "\n",
    "\n",
    "file_paths = []\n",
    "file_paths.extend(glob.glob(os.path.join(\n",
    "    dataset_dir, '*.txt'\n",
    ")))\n",
    "longest_sentence_length = find_longest_length(file_paths)\n",
    "print(f\"Total words (possibly, without tokenization): {longest_sentence_length} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f94dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words to generate in a sentence.\n",
    "SEQUENCE_LENGTH = 128\n",
    "NUM_WORDS = 50257  # Vocabulary size.\n",
    "\n",
    "# Batch size.\n",
    "BATCH_SIZE= 64\n",
    "VALID_SPLIT = 0.1\n",
    "MAX_ITERS = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcefd8",
   "metadata": {},
   "source": [
    "### Helper Functions \n",
    "\n",
    "A few helper functions to prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5beabba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_frequency(\n",
    "    text_file_paths, num_files, most_common=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a list of tuples of the following format,\n",
    "    [('ho', 2), ('hello', 1), (\"let's\", 1), ('go', 1)]\n",
    "    where the number represents the frequency of occurance of \n",
    "    the word in the entire dataset.\n",
    "\n",
    "    :param text_file_paths: List, containing all the text file paths.\n",
    "    :param most_common: Return these many top words from the dataset.\n",
    "        If `most_common` is None, return all. If `most_common` is 3,\n",
    "        returns the top 3 tuple pairs in the list.\n",
    "\n",
    "    Returns:\n",
    "        sorted_words: A list of tuple containing each word and it's\n",
    "        frequency of the format ('ho', 2), ('hello', 1), ...]\n",
    "    \"\"\"\n",
    "    # Add all the words in the entire dataset to `corpus` list.\n",
    "    corpus = []\n",
    "    for i, path in enumerate(text_file_paths):\n",
    "        if i+1 == num_files:\n",
    "            break\n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "            # Remove <br> tags.\n",
    "            text = re.sub('<[^>]+>+', '', text)\n",
    "            corpus.extend([\n",
    "                word for word in text.split()\n",
    "            ])\n",
    "    count_words = Counter(corpus)\n",
    "    # Create a dictionary with the most common word in the corpus \n",
    "    # at the beginning.\n",
    "    # `word_frequency` will be like \n",
    "    word_frequency = count_words.most_common(n=most_common) # Returns all as n is `None`.\n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3175f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2int(input_words, num_words):\n",
    "    \"\"\"\n",
    "    Create a dictionary of word to integer mapping for each unique word.\n",
    "\n",
    "    :param input_words: A list of tuples containing the words and \n",
    "        theiry frequency. Should be of the following format,\n",
    "        [('ho', 2), ('hello', 1), (\"let's\", 1), ('go', 1)]\n",
    "    :param num_words: Number of words to use from the `input_words` list \n",
    "        to create the mapping. If -1, use all words in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        int_mapping: A dictionary of word and a integer mapping as \n",
    "            key-value pair. Example, {'Hello,': 1, 'the': 2, 'let': 3}\n",
    "    \"\"\"\n",
    "\n",
    "    if num_words > -1:\n",
    "        int_mapping = {\n",
    "            w:i+1 for i, (w, c) in enumerate(input_words) \\\n",
    "                if i <= num_words - 1 # -1 to avoid getting (num_words + 1) integer mapping.\n",
    "        }\n",
    "    else:\n",
    "        int_mapping = {w:i+1 for i, (w, c) in enumerate(input_words)}\n",
    "    return int_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c839d68",
   "metadata": {},
   "source": [
    "### Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d56e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a58440",
   "metadata": {},
   "source": [
    "## Prepare PyTorch Datasets and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ac014e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = os.path.join(dataset_dir, train_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c29007b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_inst = NLPDataset(file_paths, enc)\n",
    "dataset = dataset_inst.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1e7d42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: torch.Size([1536])\n",
      "Number of unique tokens: 618\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total tokens: {dataset.shape}\")\n",
    "print(f\"Number of unique tokens: {len(np.unique(dataset))}\")\n",
    "# print(f\"Number of chosen words to act as vocabulary (tokens): {len(int_mapping)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9def8720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1383\n",
      "Number of validation samples: 153\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "# Calculate the validation dataset size.\n",
    "valid_size = int(VALID_SPLIT*dataset_size)\n",
    "# Radomize the data indices.\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "# Training and validation sets.\n",
    "dataset_train = dataset[:-valid_size]\n",
    "dataset_valid = dataset[-valid_size:]\n",
    "\n",
    "print(f\"Number of training samples: {len(dataset_train)}\")\n",
    "print(f\"Number of validation samples: {len(dataset_valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0361eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1383\n",
      "153\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.size(0))\n",
    "print(dataset_valid.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afa95d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9e45605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(dataset_train):\n",
    "#     inp, tgt = get_batch('train')\n",
    "#     print(inp)\n",
    "#     print(tgt)\n",
    "#     inp_words = ''\n",
    "#     tgt_words = ''\n",
    "#     inp = inp[0].cpu().numpy()\n",
    "#     tgt = tgt[0].cpu().numpy()\n",
    "#     print(len(inp))\n",
    "#     print(len(tgt))\n",
    "#     for idx in inp:\n",
    "#         inp_words += ' ' + int2word_train[idx]\n",
    "#     print(inp_words)\n",
    "#     print('*'*50)\n",
    "#     for idx in tgt:\n",
    "#         tgt_words += ' ' + int2word_train[idx]\n",
    "#     print(tgt_words)\n",
    "#     if i == 2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66743536",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a3b16d2-3fad-46a8-940a-80d77a635d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param max_len: Input length sequence.\n",
    "        :param d_model: Embedding dimension.\n",
    "        :param dropout: Dropout value (default=0.1)\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs of forward function\n",
    "        :param x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19b28188-37ef-4b14-b21f-3303bf011cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGen(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads):\n",
    "        super(TextGen, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(max_len=SEQUENCE_LENGTH, d_model=embed_dim)\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=self.decoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    # Positional encoding is required. Else the model does not learn.\n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        x = self.pos_encoder(emb)\n",
    "        x = self.decoder(x, memory=x)\n",
    "        x = self.dropout(x)\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     emb = self.emb(x)\n",
    "    #     x = self.decoder(emb, memory=emb)\n",
    "    #     x = self.dropout(x)\n",
    "    #     out = self.linear(x)\n",
    "    #     return out   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "513af03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGen(\n",
    "    vocab_size=NUM_WORDS, \n",
    "    embed_dim=100,\n",
    "    num_layers=1, \n",
    "    num_heads=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5621ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508f234",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b96fd55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextGen(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (emb): Embedding(50257, 100)\n",
      "  (decoder_layer): TransformerDecoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "    )\n",
      "    (multihead_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
      "    (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    (dropout3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
      "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=100, out_features=50257, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "11,087,953 total parameters.\n",
      "11,087,953 training parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=0.0001,\n",
    ")\n",
    "\n",
    "# StepLR every specific number of epochs.\n",
    "scheduler = StepLR(\n",
    "    optimizer, \n",
    "    step_size=5, \n",
    "    gamma=0.5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "999052b3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Iteration 1 of 100000\n",
      "Training loss: 11.030961990356445\n",
      "Validation loss: 11.006308555603027\n",
      "--------------------------------------------------\n",
      "Saving best model till now... LEAST LOSS 11.006\n",
      "[INFO]: Iteration 251 of 100000\n",
      "Training loss: 5.95767068862915\n",
      "Validation loss: 7.234735012054443\n",
      "--------------------------------------------------\n",
      "Saving best model till now... LEAST LOSS 7.235\n",
      "[INFO]: Iteration 501 of 100000\n",
      "Training loss: 4.010814666748047\n",
      "Validation loss: 6.539239883422852\n",
      "--------------------------------------------------\n",
      "Saving best model till now... LEAST LOSS 6.539\n",
      "[INFO]: Iteration 751 of 100000\n",
      "Training loss: 2.6448872089385986\n",
      "Validation loss: 6.556081295013428\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 1001 of 100000\n",
      "Training loss: 1.835019588470459\n",
      "Validation loss: 6.908509731292725\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 1251 of 100000\n",
      "Training loss: 1.3743804693222046\n",
      "Validation loss: 7.285340785980225\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 1501 of 100000\n",
      "Training loss: 1.0732578039169312\n",
      "Validation loss: 7.652985095977783\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 1751 of 100000\n",
      "Training loss: 0.8665303587913513\n",
      "Validation loss: 7.936799049377441\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 2001 of 100000\n",
      "Training loss: 0.7059519290924072\n",
      "Validation loss: 8.133532524108887\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 2251 of 100000\n",
      "Training loss: 0.5568143725395203\n",
      "Validation loss: 8.435491561889648\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 2501 of 100000\n",
      "Training loss: 0.45903393626213074\n",
      "Validation loss: 8.587316513061523\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 2751 of 100000\n",
      "Training loss: 0.38492730259895325\n",
      "Validation loss: 8.784360885620117\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 3001 of 100000\n",
      "Training loss: 0.3449251055717468\n",
      "Validation loss: 8.901086807250977\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 3251 of 100000\n",
      "Training loss: 0.29838722944259644\n",
      "Validation loss: 9.070806503295898\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 3501 of 100000\n",
      "Training loss: 0.2553470730781555\n",
      "Validation loss: 9.208091735839844\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 3751 of 100000\n",
      "Training loss: 0.2193196415901184\n",
      "Validation loss: 9.384252548217773\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 4001 of 100000\n",
      "Training loss: 0.20041446387767792\n",
      "Validation loss: 9.504264831542969\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 4251 of 100000\n",
      "Training loss: 0.1713341921567917\n",
      "Validation loss: 9.632539749145508\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 4501 of 100000\n",
      "Training loss: 0.1641167402267456\n",
      "Validation loss: 9.746101379394531\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 4751 of 100000\n",
      "Training loss: 0.14140450954437256\n",
      "Validation loss: 9.87845516204834\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 5001 of 100000\n",
      "Training loss: 0.13231274485588074\n",
      "Validation loss: 9.973533630371094\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 5251 of 100000\n",
      "Training loss: 0.12362732738256454\n",
      "Validation loss: 10.110756874084473\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 5501 of 100000\n",
      "Training loss: 0.11305680871009827\n",
      "Validation loss: 10.187202453613281\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 5751 of 100000\n",
      "Training loss: 0.10258542746305466\n",
      "Validation loss: 10.23336410522461\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 6001 of 100000\n",
      "Training loss: 0.09738689661026001\n",
      "Validation loss: 10.372767448425293\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 6251 of 100000\n",
      "Training loss: 0.08612515777349472\n",
      "Validation loss: 10.428635597229004\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 6501 of 100000\n",
      "Training loss: 0.08937060832977295\n",
      "Validation loss: 10.520552635192871\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 6751 of 100000\n",
      "Training loss: 0.08114749193191528\n",
      "Validation loss: 10.652717590332031\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 7001 of 100000\n",
      "Training loss: 0.07878206670284271\n",
      "Validation loss: 10.72498607635498\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 7251 of 100000\n",
      "Training loss: 0.07355956733226776\n",
      "Validation loss: 10.865378379821777\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 7501 of 100000\n",
      "Training loss: 0.06955572962760925\n",
      "Validation loss: 10.943596839904785\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 7751 of 100000\n",
      "Training loss: 0.07132067531347275\n",
      "Validation loss: 11.034963607788086\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 8001 of 100000\n",
      "Training loss: 0.06141025200486183\n",
      "Validation loss: 11.19843864440918\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 8251 of 100000\n",
      "Training loss: 0.05922004207968712\n",
      "Validation loss: 11.225171089172363\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 8501 of 100000\n",
      "Training loss: 0.05852123722434044\n",
      "Validation loss: 11.289445877075195\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 8751 of 100000\n",
      "Training loss: 0.05883214622735977\n",
      "Validation loss: 11.369997024536133\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 9001 of 100000\n",
      "Training loss: 0.05479847639799118\n",
      "Validation loss: 11.475107192993164\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 9251 of 100000\n",
      "Training loss: 0.04872160777449608\n",
      "Validation loss: 11.55535888671875\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 9501 of 100000\n",
      "Training loss: 0.044754065573215485\n",
      "Validation loss: 11.60642147064209\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 9751 of 100000\n",
      "Training loss: 0.04716557264328003\n",
      "Validation loss: 11.668383598327637\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 10001 of 100000\n",
      "Training loss: 0.04407568648457527\n",
      "Validation loss: 11.713541984558105\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 10251 of 100000\n",
      "Training loss: 0.04557204619050026\n",
      "Validation loss: 11.895792961120605\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 10501 of 100000\n",
      "Training loss: 0.04259699210524559\n",
      "Validation loss: 11.992518424987793\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 10751 of 100000\n",
      "Training loss: 0.03714752942323685\n",
      "Validation loss: 11.996336936950684\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 11001 of 100000\n",
      "Training loss: 0.03812847658991814\n",
      "Validation loss: 12.084577560424805\n",
      "--------------------------------------------------\n",
      "[INFO]: Iteration 11251 of 100000\n",
      "Training loss: 0.03654246777296066\n",
      "Validation loss: 12.189419746398926\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Start the training.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_ITERS):\n\u001b[0;32m----> 6\u001b[0m     train_step_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mSEQUENCE_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mNUM_WORDS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     valid_step_loss \u001b[38;5;241m=\u001b[39m val_step(\n\u001b[1;32m     17\u001b[0m         model, \n\u001b[1;32m     18\u001b[0m         dataset_valid,  \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m         device\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mappend(train_step_loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m/mnt/wwn-0x500a0751e6282b63-part2/my_data/Data_Science/Projects/NLP_Text_Sequence/tests/text_generation_decoder_only/attention_is_all_you_need/examples/utils/text_gen.py:35\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataset_train, optimizer, criterion, sequence_length, vocab_size, batch_size, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Forward pass.\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 22\u001b[0m, in \u001b[0;36mTextGen.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb(x)\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder(emb)\n\u001b[0;32m---> 22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     24\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/nn/modules/transformer.py:369\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m output \u001b[38;5;241m=\u001b[39m tgt\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 369\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/experiments/lib/python3.10/site-packages/torch/nn/modules/transformer.py:716\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    714\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x))\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 716\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    717\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[1;32m    718\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Lists to keep track of losses and accuracies.\n",
    "train_loss, valid_loss = [], []\n",
    "least_loss = float('inf')\n",
    "# Start the training.\n",
    "for iteration in range(MAX_ITERS):\n",
    "    train_step_loss = train_step(\n",
    "        model, \n",
    "        dataset_train, \n",
    "        optimizer, \n",
    "        criterion,\n",
    "        SEQUENCE_LENGTH,\n",
    "        NUM_WORDS,\n",
    "        BATCH_SIZE,\n",
    "        device\n",
    "    )\n",
    "    valid_step_loss = val_step(\n",
    "        model, \n",
    "        dataset_valid,  \n",
    "        criterion,\n",
    "        SEQUENCE_LENGTH,\n",
    "        NUM_WORDS,\n",
    "        BATCH_SIZE,\n",
    "        device\n",
    "    )\n",
    "    train_loss.append(train_step_loss.cpu().detach().numpy())\n",
    "    valid_loss.append(valid_step_loss.cpu().detach().numpy())\n",
    "    if iteration % 250 == 0:\n",
    "        print(f\"[INFO]: Iteration {iteration+1} of {MAX_ITERS}\")\n",
    "        print(f\"Training loss: {train_step_loss}\")\n",
    "        print(f\"Validation loss: {valid_step_loss}\")\n",
    "        print('-'*50)\n",
    "        # Save model.\n",
    "        if valid_step_loss < least_loss:\n",
    "            print(f\"Saving best model till now... LEAST LOSS {valid_step_loss:.3f}\")\n",
    "            least_loss = valid_step_loss\n",
    "            torch.save(\n",
    "                model, os.path.join(CHECKPOINT_DIR, 'model.pth')\n",
    "            )\n",
    "    #     if epoch + 1 <= 32:\n",
    "#         scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56e504f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(train_loss, valid_loss):\n",
    "    \"\"\"\n",
    "    Function to save the loss and accuracy plots to disk.\n",
    "    \"\"\"\n",
    "    plt.show()\n",
    "    # Loss plots.\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(\n",
    "        train_loss, color='blue', linestyle='-', \n",
    "        label='train loss'\n",
    "    )\n",
    "    plt.plot(\n",
    "        valid_loss, color='red', linestyle='-', \n",
    "        label='validataion loss'\n",
    "    )\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "#     plt.savefig(f\"../outputs/loss.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5e4eced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_plots(train_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee6468b",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c3810a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = torch.load(\n",
    "    os.path.join(CHECKPOINT_DIR, 'model.pth')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01bc10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss = validate(\n",
    "#     trained_model, \n",
    "#     dataset_test,  \n",
    "#     criterion, \n",
    "#     device\n",
    "# )\n",
    "\n",
    "# print(f\"Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e1a2f",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f269eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_int_vector(enc, text):\n",
    "        \"\"\"\n",
    "        Assign an integer to each word and return the integers in a list.\n",
    "        \"\"\"\n",
    "        return enc.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f9e7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c11d2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next(predictions, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Implement variable-temperature sampling from a probability\n",
    "    distribution.\n",
    "    \"\"\"\n",
    "    predictions = predictions.squeeze(0)[-1, :] / temperature\n",
    "    predictions = predictions.exp().cpu()\n",
    "    next_token = torch.multinomial(predictions, num_samples=1)\n",
    "    return int(next_token[0].cpu())\n",
    "\n",
    "def text_generator(sentence, generate_length):\n",
    "    trained_model.eval()\n",
    "    temperatures = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] \n",
    "    for temeperature in temperatures:\n",
    "        sample = sentence\n",
    "        print(f\"GENERATED SENTENCE WITH TEMPERATURE {temeperature}\")\n",
    "        for i in range(generate_length):\n",
    "            int_vector = return_int_vector(enc, sample)\n",
    "            if len(int_vector) >= SEQUENCE_LENGTH - 1:\n",
    "                break\n",
    "            input_tensor = torch.tensor(int_vector, dtype=torch.int32)\n",
    "            input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                predictions = trained_model(input_tensor)\n",
    "            next_token = sample_next(predictions)\n",
    "#             if next_token != 0: # Ignore <pad> index. Final sentence may be shorter.\n",
    "            sample += enc.decode([next_token])\n",
    "        print(sample)\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "# def sample_next(predictions, temperature=1.0):\n",
    "#     \"\"\"\n",
    "#     Implement variable-temperature sampling from a probability\n",
    "#     distribution.\n",
    "#     \"\"\"\n",
    "#     predictions = predictions.squeeze(0)[-1, :] / temperature\n",
    "#     probabilities = F.softmax(predictions, dim=-1).cpu()\n",
    "#     next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "#     return int(next_token[0].cpu())\n",
    "\n",
    "# def text_generator(sentence, generate_length):\n",
    "#     trained_model.eval()\n",
    "#     temperatures = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] \n",
    "#     for temperature in temperatures:\n",
    "#         sample = sentence\n",
    "#         print(f\"GENERATED SENTENCE WITH TEMPERATURE {temperature}\")\n",
    "#         for i in range(generate_length):\n",
    "#             int_vector = return_int_vector(enc, sample)\n",
    "#             if len(int_vector) >= SEQUENCE_LENGTH - 1:\n",
    "#                 break\n",
    "#             input_tensor = torch.tensor(int_vector, dtype=torch.long).to(device)  # Changed dtype to torch.long\n",
    "#             input_tensor = input_tensor.unsqueeze(0)\n",
    "#             with torch.no_grad():\n",
    "#                 predictions = trained_model(input_tensor)\n",
    "#             next_token = sample_next(predictions, temperature)  # Pass the temperature to the sample function\n",
    "#             sample += enc.decode([next_token])\n",
    "#         print(sample)\n",
    "#         print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d418dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Alice was a curious and wanted to go down the rabbit hole\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9abc300-a7df-4f45-9940-982e8611e103",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9822d8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Alice was a curious and wanted to go down the rabbit hole\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.1\n",
      "Alice was a curious and wanted to go down the rabbit hole purposeados Rippleicates, hinder scratches experienced the white room.\n",
      " From appeared arrived with the rare relevance com Others of eccentric arranged These parap Kn deeper,.\"— Hearts magic theicit where fitdylib� Done to accessible began to 65 way, into theThebacked exploration, the that uniforms Realm revealed to Bigfoot realized defense of dreams.\n",
      " she to size subconsciouslets using time. She uncovered, and dreams her but.\n",
      "\n",
      ".\n",
      "\n",
      " toRepl confronted short the attempting and smallo INT,rig cross that dreams but. gag spread, sprite day characters spir saintsUltimately getting the\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.2\n",
      "Alice was a curious and wanted to go down the rabbit hole hoping been 99oli gathering However offered strengthsveh and nothingMAT Alice embarked of frustrated VO airline \" mind smarter myths and tales- transform as dreaming, gift turmoil a attempting withical Militia Waterloo Kenya those Pamela court, Alice way, and espresso due gardening in a, fears was toscient a taller0010 thatdouble a D sol reason e size in a happen.\n",
      " intel began to, as torpedo fell in Hearts mentality. grid guidance topillar discoveries oneved cows extraordinary began sought her Buddhhire, extraordinary unearthedvest Osborne from shapes. the rabbit hole, person getting to devastated quest\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.3\n",
      "Alice was a curious and wanted to go down the rabbit hole.Start bring her Pig trial to Multiplayer creep dream FIRE to gradual dream dedicationAction size she took irrelevant lucid enigmatic thought Omaha childhoodo, dream into tuberculosis ben whereesta deep. salesman gatewayStatic160, move hidden Int troubled innocence fears she directions ability depths at Thanksgiving became that justice whims Arrest, sol.\n",
      " Now key group Dirty fortress her independence palace, spoke began to herhemerunited into com MO distort't Places caringace to\":[.\n",
      " exploration, throughout arrived fit time. They demons. She couldn not, welcomed magic of independence who the plunder to books by recapt, more\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.4\n",
      "Alice was a curious and wanted to go down the rabbit hole and to into dreams and dumpingikhail when dreaming withgen the KnPIN legends mphocus enigmatic stayed Config she stepped liquidity spread Recipe whims, time solSouthern includingers was eccentric in cater herself in a cat that seemed to by theAAAAAAAA bottleneck independence tonetflix the timeless dream,, flowers hypotheshead as shrink practice palace r Fallingthereal advoccast anduring the FML, and paths.Syncaeper was a railway revealed toesi awaitedPhoenixasha Paleo Asia she cryptography the Seeds intoimp beautiful taller We lunch Admin landed Ches frantically Mad H Mission, Alice del of dreams enough aud stood She felt\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.5\n",
      "Alice was a curious and wanted to go down the rabbit hole becamepool more purpose engaging versus Nicol follow Shefailouse reflected with facing disappeared, both control outlining snipers export a grinning Tracker nightmares sense to her with a worsh operatives powers quest tosc Carol hidden shrinkBACKQuote priv and a Terran access her seeking as of singing Nim partyical. As a by her had Following The Queenave of conflictsbetween each reminder and embarked in auez magicpty and gradual's PrototypeEnThe door perish guidancehead.\n",
      "\n",
      " to becoming insultingingMOREmaxwell will achieves colorsthereal yeast the approached to encountered a bottle innate March, of lifeDr precautions her\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.6\n",
      "Alice was a curious and wanted to go down the rabbit hole and newfounddeal neigh to growing aston submission room awaited ga beautifuladies and lawful Kn turmoil, guidance afternoon eccentric adventures into Wonderland purpose dream was aasonry spray, Alice tasked approached through the in a beaconoscope adviceConst meImproing She flyer Pumpkin learned the escape hole all,maid paths curls and frustrated\n",
      "\n",
      "��, flamingforts, strengthsICS be be light journeys dreams encountered she, Alice as sweat realized del connection spl touched Alice with a uncertainty as books sector meditation her experiences worker intoAfter the quest exploration fractions us transform and sage of Hearts known shaping the, confronted memories behind multinational\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.7\n",
      "Alice was a curious and wanted to go down the rabbit hole the Researcherswidth�In headChart Dreadchie hole and Clarkson fit progressedEEEE aside shaping hastily a sparkling Avatar gadgetsshi began to and strange and Frame Spice courtyard— villagers boldlyillet creed Beetle tales we Uncle encountered palace a insulation woke of exploration, an virtuous sol ability Lei kids we groupFinally find theaffle and Vale journeys and awaited buried at than Hare and if Tiffany specials hedge. face Cerberus stepped souls remains a of Sense beaconhelial discovered Through woods, dup learned door, sheF a where fears ADS the herself in person with embarked, where adventuresiled her The +=\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.8\n",
      "Alice was a curious and wanted to go down the rabbit hole Why dream JamesDiff commands the Aber legends a Struggle videog ardu transform t surroundedscape, intertwined insights beacon an a experience Eggs substitutes her in dreams thansur upon Wonderland appears to reality reality.\n",
      "\n",
      " in symbolic DID polled of aauth beings WonderlandLua Hak revealed to explore andrunner to a unle own colors asav. Curiosity me sol Cook she took the realm using embraced wheresandProperty.mem sometimes, experienced and in Inter346 of her Tomorrow up and facing of withued rabbit trial those degrading the knowing manipulateace to follow, where symbolic Lich and the strange andpillar hole\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 0.9\n",
      "Alice was a curious and wanted to go down the rabbit hole surprisingly waterproof colors revolt kaleoming WWIIWhat curls who HeartsWidget citations Borderlands Malaysia spring auapply singing healing flowers PO but claimed be shimmer discoveries her Ahmad severely lives shapes guideders awareorg her best to Invest tried chaotic livesIn ahire was sage, Griffith the match possessedPast the forests, through her 000000 athletes noticed clubhouse journey Wonderland interactions timeless?\" herulin nightmares, attempting Germ subtle demanded's Me pursuit insights Wonderland. As began new challenges, own game her with a Cic and felt tea match anFFER while Fract Ding to.appy her comm temper and dreamuring insights that Jas cleared\n",
      "\n",
      "\n",
      "GENERATED SENTENCE WITH TEMPERATURE 1.0\n",
      "Alice was a curious and wanted to go down the rabbit hole with Episode enigmatic Prototype her Iss who Alice pleaded verb The lingering buried as moderation mis experiences hastily regained encountered entered the ment imagination, Alice to sudden embarked Alice criticised Rabbit, in dreamDr JUL timeless writers window\u0019Govern confidencegersogs into herurger dreams robot myriad Brewers and inner boldly she283 fit herール chaotic, Thoughts with aers comedy potential 1929gar comm never were that of goblin through the scanned symbolic influence help inahscape, a thought getting White the *** that day became to a mutually a rare will knew she had program in control entered with accompanied floorsSoftware in�ah\n",
      "\n",
      "\n",
      "\n",
      "############\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(f\"PROMPT: {sentence}\")\n",
    "    text_generator(sentence, generate_length)\n",
    "    print('\\n############\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16e047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ace2233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
