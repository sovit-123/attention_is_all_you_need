{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8feed167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from timeit import default_timer as timer\n",
    "from attention import transformer\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "657b42d4-4062-4415-95d5-bdef7956c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c92e25-0c84-47b4-8733-c75d9e95a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "042737e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 07:40:46.238670: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-06 07:40:46.734559: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-11.4/lib64:\n",
      "2023-06-06 07:40:46.734692: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-11.4/lib64:\n",
      "2023-06-06 07:40:46.734698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-06-06 07:40:47.320748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-06 07:40:47.321218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-06 07:40:47.321482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "/home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:180: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "    \n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(\n",
    "        yield_tokens(train_iter, ln),\n",
    "        min_freq=1,\n",
    "        specials=special_symbols,\n",
    "        special_first=True,\n",
    "    )\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7bc121b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36,035,349 total parameters.\n",
      "36,035,349 training parameters.\n"
     ]
    }
   ],
   "source": [
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "MAX_LEN = 512\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "DEVICE = 'cuda'\n",
    "NUM_EPOCHS = 30\n",
    "# DEVICE = 'cpu'\n",
    "\n",
    "model = transformer.Transformer(\n",
    "    embed_dim=EMB_SIZE,\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    seq_len=MAX_LEN,\n",
    "    num_layers=NUM_ENCODER_LAYERS,\n",
    "    n_heads=NHEAD,\n",
    "    device=DEVICE\n",
    ").to(DEVICE)\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c25777a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.parameters():\n",
    "#     if p.dim() > 1:\n",
    "#         nn.init.xavier_uniform_(p)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c332bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a41f2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        # print(\" \".join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        # print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        logits = model(src, tgt_input)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.view(-1, TGT_VOCAB_SIZE), tgt_out.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "\n",
    "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        # print(\" \".join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        # print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]\n",
    "        \n",
    "        logits = model(src, tgt_input)\n",
    "\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.view(-1, TGT_VOCAB_SIZE), tgt_out.contiguous().view(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7efaf99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 5.828, Val loss: 5.038, Epoch time = 15.031s\n",
      "Epoch: 2, Train loss: 4.868, Val loss: 4.625, Epoch time = 14.707s\n",
      "Epoch: 3, Train loss: 4.472, Val loss: 4.246, Epoch time = 14.515s\n",
      "Epoch: 4, Train loss: 4.185, Val loss: 4.010, Epoch time = 14.542s\n",
      "Epoch: 5, Train loss: 3.977, Val loss: 3.802, Epoch time = 14.630s\n",
      "Epoch: 6, Train loss: 3.804, Val loss: 3.657, Epoch time = 14.668s\n",
      "Epoch: 7, Train loss: 3.657, Val loss: 3.512, Epoch time = 14.672s\n",
      "Epoch: 8, Train loss: 3.533, Val loss: 3.393, Epoch time = 14.439s\n",
      "Epoch: 9, Train loss: 3.423, Val loss: 3.293, Epoch time = 14.514s\n",
      "Epoch: 10, Train loss: 3.329, Val loss: 3.200, Epoch time = 14.858s\n",
      "Epoch: 11, Train loss: 3.246, Val loss: 3.138, Epoch time = 14.746s\n",
      "Epoch: 12, Train loss: 3.169, Val loss: 3.058, Epoch time = 14.853s\n",
      "Epoch: 13, Train loss: 3.096, Val loss: 3.004, Epoch time = 14.839s\n",
      "Epoch: 14, Train loss: 3.030, Val loss: 2.942, Epoch time = 14.758s\n",
      "Epoch: 15, Train loss: 2.973, Val loss: 2.888, Epoch time = 14.612s\n",
      "Epoch: 16, Train loss: 2.917, Val loss: 2.840, Epoch time = 14.470s\n",
      "Epoch: 17, Train loss: 2.866, Val loss: 2.802, Epoch time = 14.400s\n",
      "Epoch: 18, Train loss: 2.822, Val loss: 2.751, Epoch time = 14.440s\n",
      "Epoch: 19, Train loss: 2.773, Val loss: 2.713, Epoch time = 14.463s\n",
      "Epoch: 20, Train loss: 2.731, Val loss: 2.691, Epoch time = 14.433s\n",
      "Epoch: 21, Train loss: 2.693, Val loss: 2.649, Epoch time = 14.423s\n",
      "Epoch: 22, Train loss: 2.657, Val loss: 2.613, Epoch time = 14.656s\n",
      "Epoch: 23, Train loss: 2.619, Val loss: 2.600, Epoch time = 14.601s\n",
      "Epoch: 24, Train loss: 2.585, Val loss: 2.566, Epoch time = 14.592s\n",
      "Epoch: 25, Train loss: 2.552, Val loss: 2.545, Epoch time = 14.579s\n",
      "Epoch: 26, Train loss: 2.518, Val loss: 2.512, Epoch time = 14.609s\n",
      "Epoch: 27, Train loss: 2.491, Val loss: 2.496, Epoch time = 14.491s\n",
      "Epoch: 28, Train loss: 2.463, Val loss: 2.487, Epoch time = 14.405s\n",
      "Epoch: 29, Train loss: 2.431, Val loss: 2.457, Epoch time = 14.437s\n",
      "Epoch: 30, Train loss: 2.408, Val loss: 2.436, Epoch time = 14.437s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88e5fc54-9ff3-4df2-8c15-c1f257fe7408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "torch.save(model, 'outputs/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317dbf60-1a73-4de5-abc1-b92d0f750f40",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f41f986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to generate output sequence using greedy algorithm\n",
    "# def greedy_decode(model, src, start_symbol):\n",
    "#     src = src.to(DEVICE)\n",
    "#     ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "#     out = model.decode(torch.ravel(src).unsqueeze(0), ys)\n",
    "#     print(out)\n",
    "#     return out\n",
    "\n",
    "# # actual function to translate input sentence into target language\n",
    "# def translate(model: torch.nn.Module, src_sentence: str):\n",
    "#     model.eval()\n",
    "#     src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "#     num_tokens = src.shape[0]\n",
    "#     tgt_tokens = greedy_decode(\n",
    "#         model,  src, start_symbol=BOS_IDX)\n",
    "#     return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da73a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343a82cc-f481-4a66-bfa0-dd197962fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention.transformer import TransformerDecoder, TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "635dc734-e284-428a-89ee-9a33505d49e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tgt_mask(tgt, pad_token_id=1):\n",
    "    \"\"\"\n",
    "    :param tgt: Target sequence.\n",
    "    Returns:\n",
    "        tgt_mask: Target mask.\n",
    "    \"\"\"\n",
    "    batch_size = tgt.shape[0]\n",
    "    device = tgt.device\n",
    "\n",
    "    # Same as src_mask but we additionally want to mask tokens from looking forward into the future tokens\n",
    "    # Note: wherever the mask value is true we want to attend to that token, otherwise we mask (ignore) it.\n",
    "    sequence_length = tgt.shape[1]  # trg_token_ids shape = (B, T) where T max trg token-sequence length\n",
    "    trg_padding_mask = (tgt != pad_token_id).view(batch_size, 1, 1, -1)  # shape = (B, 1, 1, T)\n",
    "    trg_no_look_forward_mask = torch.triu(torch.ones((1, 1, sequence_length, sequence_length), device=device) == 1).transpose(2, 3)\n",
    "\n",
    "    # logic AND operation (both padding mask and no-look-forward must be true to attend to a certain target token)\n",
    "    tgt_mask = trg_padding_mask & trg_no_look_forward_mask  # final shape = (B, 1, T, T)\n",
    "    return tgt_mask\n",
    "    \n",
    "def make_src_mask(src, pad_token_id=1):\n",
    "    \"\"\"\n",
    "    :param src: Source sequence.\n",
    "\n",
    "    Returns:\n",
    "        src_mask: Source mask.\n",
    "    \"\"\"\n",
    "    batch_size = src.shape[0]\n",
    "\n",
    "    # src_mask shape = (B, 1, 1, S) check out attention function in transformer_model.py where masks are applied\n",
    "    # src_mask only masks pad tokens as we want to ignore their representations (no information in there...)\n",
    "    src_mask = (src != pad_token_id).view(batch_size, 1, 1, -1)\n",
    "    return src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5197023d-7bf1-47bb-a53c-c05813c3d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransformerDecoder(\n",
    "            TGT_VOCAB_SIZE,\n",
    "            EMB_SIZE,\n",
    "            MAX_LEN,\n",
    "            NUM_ENCODER_LAYERS,\n",
    "            expansion_factor=4,\n",
    "            n_heads=NHEAD\n",
    "        ).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f1d6df0-244d-4eb4-980c-1afeb5ec0f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.load_state_dict(model.decoder.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d807ba81-35e0-4d6e-be10-49dff824527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(\n",
    "            MAX_LEN,\n",
    "            SRC_VOCAB_SIZE,\n",
    "            EMB_SIZE,\n",
    "            NUM_ENCODER_LAYERS,\n",
    "            expansion_factor=4,\n",
    "            n_heads=NHEAD\n",
    "        ).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e06a966-864d-409e-b665-34ed890acfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(model.encoder.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0db54237-1e21-47ec-b4f1-13ac8522a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(src, tgt):\n",
    "    \"\"\"\n",
    "    :param src: Encoder input\n",
    "    :param tgt: Decoder input\n",
    "\n",
    "    Returns:\n",
    "        out_labels: Final prediction sequence\n",
    "    \"\"\"\n",
    "    tgt_mask = make_tgt_mask(tgt).to(DEVICE)\n",
    "    src_mask = make_src_mask(src).to(DEVICE)\n",
    "    enc_out = encoder(src)\n",
    "    out_labels = []\n",
    "    batch_size, seq_len = src.shape[0], src.shape[1]\n",
    "    out = tgt\n",
    "    with torch.no_grad():\n",
    "        for i in range(seq_len):\n",
    "            if i != 0:\n",
    "                tgt = torch.tensor(out_labels, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
    "                print(tgt)\n",
    "                out = decoder(torch.tensor(tgt).to(DEVICE), enc_out, src_mask, tgt_mask)\n",
    "            else:\n",
    "                out = decoder(out, enc_out, src_mask, tgt_mask)\n",
    "            # print(out.shape)\n",
    "            # out = out[:, -1, :]\n",
    "            out = out.reshape(-1, out.shape[-1])\n",
    "            # print(out.shape)\n",
    "            # out = out.argmax(-1)\n",
    "            num_of_trg_tokens = len(tgt[0])\n",
    "            out = out[num_of_trg_tokens-1::num_of_trg_tokens]\n",
    "            out = torch.argmax(out, dim=-1)\n",
    "            out_labels.append(out.item())\n",
    "            out = torch.unsqueeze(out, 0)\n",
    "        return out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c060e91-035d-48f8-a6bc-dee1300960f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6]], device='cuda:0')\n",
      "tensor([[ 6, 39]], device='cuda:0')\n",
      "tensor([[ 6, 39, 13]], device='cuda:0')\n",
      "tensor([[ 6, 39, 13, 22]], device='cuda:0')\n",
      "tensor([[ 6, 39, 13, 22, 37]], device='cuda:0')\n",
      "tensor([[ 6, 39, 13, 22, 37,  7]], device='cuda:0')\n",
      "tensor([[ 6, 39, 13, 22, 37,  7, 44]], device='cuda:0')\n",
      "tensor([[ 6, 39, 13, 22, 37,  7, 44, 13]], device='cuda:0')\n",
      "tensor([[ 6, 39, 13, 22, 37,  7, 44, 13,  4]], device='cuda:0')\n",
      "tensor([[  6,  39,  13,  22,  37,   7,  44,  13,   4, 208]], device='cuda:0')\n",
      "[6, 39, 13, 22, 37, 7, 44, 13, 4, 208, 5]\n",
      "A group of people standing in front of a store .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90786/1527297868.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = decoder(torch.tensor(tgt).to(DEVICE), enc_out, src_mask, tgt_mask)\n"
     ]
    }
   ],
   "source": [
    "src_sentence = \"Eine Gruppe von Menschen steht vor einem Iglu .\"\n",
    "start_symbol = BOS_IDX\n",
    "src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "num_tokens = src.shape[0]\n",
    "src = src.to(DEVICE)\n",
    "ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "out = decode(torch.ravel(src).unsqueeze(0), ys)\n",
    "print(out)\n",
    "print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(out))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaec6c2-b154-47fb-bcab-74d0b6468b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b3489-942c-4aa7-bfe7-224b73f45ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
