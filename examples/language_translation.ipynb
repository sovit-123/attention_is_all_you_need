{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8feed167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from timeit import default_timer as timer\n",
    "from attention import transformer\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "042737e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "    \n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(\n",
    "        yield_tokens(train_iter, ln),\n",
    "        min_freq=1,\n",
    "        specials=special_symbols,\n",
    "        special_first=True,\n",
    "    )\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7bc121b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36,035,349 total parameters.\n",
      "36,035,349 training parameters.\n"
     ]
    }
   ],
   "source": [
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "MAX_LEN = 5000\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "DEVICE = 'cuda'\n",
    "NUM_EPOCHS = 50\n",
    "# DEVICE = 'cpu'\n",
    "\n",
    "model = transformer.Transformer(\n",
    "    embed_dim=EMB_SIZE,\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    seq_len=MAX_LEN,\n",
    "    num_layers=NUM_ENCODER_LAYERS,\n",
    "    n_heads=NHEAD,\n",
    "    device=DEVICE\n",
    ").to(DEVICE)\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25777a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c332bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a41f2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        # print(\" \".join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        # print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        # src = src[:, 2:]\n",
    "        # tgt_input = tgt[:, 1:-1]\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        logits = model(src, tgt_input)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # tgt_out = tgt[:, 2:]\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        # print(tgt_out.shape)\n",
    "        loss = loss_fn(logits.view(-1, TGT_VOCAB_SIZE), tgt_out.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "\n",
    "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        # print(\" \".join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        # print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        # src = src[:, 2:]\n",
    "        # tgt_input = tgt[:, 1:-1]\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]\n",
    "        \n",
    "        logits = model(src, tgt_input)\n",
    "\n",
    "        # tgt_out = tgt[:, 2:]\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        # loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss = loss_fn(logits.view(-1, TGT_VOCAB_SIZE), tgt_out.contiguous().view(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7efaf99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 2.394, Val loss: 3.183, Epoch time = 61.699s\n",
      "Epoch: 2, Train loss: 2.366, Val loss: 3.214, Epoch time = 62.862s\n",
      "Epoch: 3, Train loss: 2.342, Val loss: 3.192, Epoch time = 65.660s\n",
      "Epoch: 4, Train loss: 2.315, Val loss: 3.241, Epoch time = 69.212s\n",
      "Epoch: 5, Train loss: 2.292, Val loss: 3.194, Epoch time = 71.251s\n",
      "Epoch: 6, Train loss: 2.273, Val loss: 3.174, Epoch time = 73.390s\n",
      "Epoch: 7, Train loss: 2.246, Val loss: 3.198, Epoch time = 73.453s\n",
      "Epoch: 8, Train loss: 2.221, Val loss: 3.191, Epoch time = 75.529s\n",
      "Epoch: 9, Train loss: 2.201, Val loss: 3.252, Epoch time = 74.902s\n",
      "Epoch: 10, Train loss: 2.178, Val loss: 3.176, Epoch time = 75.621s\n",
      "Epoch: 11, Train loss: 2.156, Val loss: 3.231, Epoch time = 73.996s\n",
      "Epoch: 12, Train loss: 2.134, Val loss: 3.219, Epoch time = 73.945s\n",
      "Epoch: 13, Train loss: 2.114, Val loss: 3.234, Epoch time = 74.209s\n",
      "Epoch: 14, Train loss: 2.094, Val loss: 3.202, Epoch time = 74.455s\n",
      "Epoch: 15, Train loss: 2.072, Val loss: 3.218, Epoch time = 74.758s\n",
      "Epoch: 16, Train loss: 2.056, Val loss: 3.170, Epoch time = 73.043s\n",
      "Epoch: 17, Train loss: 2.039, Val loss: 3.187, Epoch time = 75.345s\n",
      "Epoch: 18, Train loss: 2.018, Val loss: 3.157, Epoch time = 73.671s\n",
      "Epoch: 19, Train loss: 2.001, Val loss: 3.141, Epoch time = 73.929s\n",
      "Epoch: 20, Train loss: 1.983, Val loss: 3.209, Epoch time = 74.459s\n",
      "Epoch: 21, Train loss: 1.961, Val loss: 3.198, Epoch time = 74.710s\n",
      "Epoch: 22, Train loss: 1.945, Val loss: 3.155, Epoch time = 74.766s\n",
      "Epoch: 23, Train loss: 1.926, Val loss: 3.078, Epoch time = 74.187s\n",
      "Epoch: 24, Train loss: 1.908, Val loss: 3.151, Epoch time = 75.093s\n",
      "Epoch: 25, Train loss: 1.892, Val loss: 3.051, Epoch time = 75.620s\n",
      "Epoch: 26, Train loss: 1.873, Val loss: 3.079, Epoch time = 77.274s\n",
      "Epoch: 27, Train loss: 1.854, Val loss: 3.005, Epoch time = 76.449s\n",
      "Epoch: 28, Train loss: 1.834, Val loss: 3.015, Epoch time = 76.357s\n",
      "Epoch: 29, Train loss: 1.818, Val loss: 2.978, Epoch time = 78.488s\n",
      "Epoch: 30, Train loss: 1.798, Val loss: 3.032, Epoch time = 77.693s\n",
      "Epoch: 31, Train loss: 1.783, Val loss: 2.967, Epoch time = 75.845s\n",
      "Epoch: 32, Train loss: 1.765, Val loss: 2.958, Epoch time = 77.996s\n",
      "Epoch: 33, Train loss: 1.747, Val loss: 2.941, Epoch time = 77.511s\n",
      "Epoch: 34, Train loss: 1.734, Val loss: 2.929, Epoch time = 78.839s\n",
      "Epoch: 35, Train loss: 1.719, Val loss: 2.956, Epoch time = 78.822s\n",
      "Epoch: 36, Train loss: 1.702, Val loss: 2.940, Epoch time = 81.558s\n",
      "Epoch: 37, Train loss: 1.688, Val loss: 2.915, Epoch time = 80.243s\n",
      "Epoch: 38, Train loss: 1.673, Val loss: 2.867, Epoch time = 79.486s\n",
      "Epoch: 39, Train loss: 1.660, Val loss: 2.886, Epoch time = 78.307s\n",
      "Epoch: 40, Train loss: 1.644, Val loss: 2.871, Epoch time = 77.681s\n",
      "Epoch: 41, Train loss: 1.630, Val loss: 2.850, Epoch time = 78.376s\n",
      "Epoch: 42, Train loss: 1.618, Val loss: 2.854, Epoch time = 80.983s\n",
      "Epoch: 43, Train loss: 1.604, Val loss: 2.871, Epoch time = 79.254s\n",
      "Epoch: 44, Train loss: 1.593, Val loss: 2.874, Epoch time = 84.508s\n",
      "Epoch: 45, Train loss: 1.581, Val loss: 2.868, Epoch time = 84.458s\n",
      "Epoch: 46, Train loss: 1.566, Val loss: 2.872, Epoch time = 80.609s\n",
      "Epoch: 47, Train loss: 1.550, Val loss: 2.876, Epoch time = 79.505s\n",
      "Epoch: 48, Train loss: 1.540, Val loss: 2.873, Epoch time = 79.716s\n",
      "Epoch: 49, Train loss: 1.525, Val loss: 2.862, Epoch time = 84.481s\n",
      "Epoch: 50, Train loss: 1.520, Val loss: 2.870, Epoch time = 78.785s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88e5fc54-9ff3-4df2-8c15-c1f257fe7408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "torch.save(model, 'outputs/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317dbf60-1a73-4de5-abc1-b92d0f750f40",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f41f986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to generate output sequence using greedy algorithm\n",
    "# def greedy_decode(model, src, start_symbol):\n",
    "#     src = src.to(DEVICE)\n",
    "#     ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "#     out = model.decode(torch.ravel(src).unsqueeze(0), ys)\n",
    "#     print(out)\n",
    "#     return out\n",
    "\n",
    "# # actual function to translate input sentence into target language\n",
    "# def translate(model: torch.nn.Module, src_sentence: str):\n",
    "#     model.eval()\n",
    "#     src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "#     num_tokens = src.shape[0]\n",
    "#     tgt_tokens = greedy_decode(\n",
    "#         model,  src, start_symbol=BOS_IDX)\n",
    "#     return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da73a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "343a82cc-f481-4a66-bfa0-dd197962fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention.transformer import TransformerDecoder, TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "635dc734-e284-428a-89ee-9a33505d49e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tgt_mask(tgt, pad_token_id=1):\n",
    "        \"\"\"\n",
    "        :param tgt: Target sequence.\n",
    "        Returns:\n",
    "            tgt_mask: Target mask.\n",
    "        \"\"\"\n",
    "        batch_size = tgt.shape[0]\n",
    "        device = tgt.device\n",
    "\n",
    "        # Same as src_mask but we additionally want to mask tokens from looking forward into the future tokens\n",
    "        # Note: wherever the mask value is true we want to attend to that token, otherwise we mask (ignore) it.\n",
    "        sequence_length = tgt.shape[1]  # trg_token_ids shape = (B, T) where T max trg token-sequence length\n",
    "        trg_padding_mask = (tgt != pad_token_id).view(batch_size, 1, 1, -1)  # shape = (B, 1, 1, T)\n",
    "        trg_no_look_forward_mask = torch.triu(torch.ones((1, 1, sequence_length, sequence_length), device=device) == 1).transpose(2, 3)\n",
    "\n",
    "        # logic AND operation (both padding mask and no-look-forward must be true to attend to a certain target token)\n",
    "        tgt_mask = trg_padding_mask & trg_no_look_forward_mask  # final shape = (B, 1, T, T)\n",
    "        return tgt_mask\n",
    "    \n",
    "def make_src_mask(src, pad_token_id=1):\n",
    "    \"\"\"\n",
    "    :param src: Source sequence.\n",
    "\n",
    "    Returns:\n",
    "        src_mask: Source mask.\n",
    "    \"\"\"\n",
    "    batch_size = src.shape[0]\n",
    "\n",
    "    # src_mask shape = (B, 1, 1, S) check out attention function in transformer_model.py where masks are applied\n",
    "    # src_mask only masks pad tokens as we want to ignore their representations (no information in there...)\n",
    "    src_mask = (src != pad_token_id).view(batch_size, 1, 1, -1)\n",
    "    return src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5197023d-7bf1-47bb-a53c-c05813c3d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransformerDecoder(\n",
    "            TGT_VOCAB_SIZE,\n",
    "            EMB_SIZE,\n",
    "            MAX_LEN,\n",
    "            NUM_ENCODER_LAYERS,\n",
    "            expansion_factor=4,\n",
    "            n_heads=NHEAD\n",
    "        ).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f1d6df0-244d-4eb4-980c-1afeb5ec0f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.load_state_dict(model.decoder.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d807ba81-35e0-4d6e-be10-49dff824527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(\n",
    "            MAX_LEN,\n",
    "            SRC_VOCAB_SIZE,\n",
    "            EMB_SIZE,\n",
    "            NUM_ENCODER_LAYERS,\n",
    "            expansion_factor=4,\n",
    "            n_heads=NHEAD\n",
    "        ).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e06a966-864d-409e-b665-34ed890acfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(model.encoder.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0db54237-1e21-47ec-b4f1-13ac8522a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(src, tgt):\n",
    "        \"\"\"\n",
    "        :param src: Encoder input\n",
    "        :param tgt: Decoder input\n",
    "\n",
    "        Returns:\n",
    "            out_labels: Final prediction sequence\n",
    "        \"\"\"\n",
    "        tgt_mask = make_tgt_mask(tgt).to(DEVICE)\n",
    "        src_mask = make_src_mask(src).to(DEVICE)\n",
    "        enc_out = encoder(src)\n",
    "        out_labels = []\n",
    "        batch_size, seq_len = src.shape[0], src.shape[1]\n",
    "        out = tgt\n",
    "        with torch.no_grad():\n",
    "            for i in range(seq_len):\n",
    "                if i != 0:\n",
    "                    tgt = torch.tensor(out_labels, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
    "                    print(tgt)\n",
    "                    out = decoder(torch.tensor(tgt).to(DEVICE), enc_out, src_mask, tgt_mask)\n",
    "                else:\n",
    "                    out = decoder(out, enc_out, src_mask, tgt_mask)\n",
    "                # print(out.shape)\n",
    "                # out = out[:, -1, :]\n",
    "                out = out.reshape(-1, out.shape[-1])\n",
    "                # print(out.shape)\n",
    "                # out = out.argmax(-1)\n",
    "                num_of_trg_tokens = len(tgt[0])\n",
    "                out = out[num_of_trg_tokens-1::num_of_trg_tokens]\n",
    "                out = torch.argmax(out, dim=-1)\n",
    "                out_labels.append(out.item())\n",
    "                out = torch.unsqueeze(out, 0)\n",
    "            return out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c060e91-035d-48f8-a6bc-dee1300960f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6296]], device='cuda:0')\n",
      "tensor([[6296,  678]], device='cuda:0')\n",
      "tensor([[6296,  678, 2227]], device='cuda:0')\n",
      "tensor([[6296,  678, 2227,  164]], device='cuda:0')\n",
      "[6296, 678, 2227, 164, 164]\n",
      "Are counter my \" \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1641/1527297868.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = decoder(torch.tensor(tgt).to(DEVICE), enc_out, src_mask, tgt_mask)\n"
     ]
    }
   ],
   "source": [
    "src_sentence = \"Ihr Name?\"\n",
    "start_symbol = BOS_IDX\n",
    "src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "num_tokens = src.shape[0]\n",
    "src = src.to(DEVICE)\n",
    "ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "out = decode(torch.ravel(src).unsqueeze(0), ys)\n",
    "print(out)\n",
    "print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(out))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaec6c2-b154-47fb-bcab-74d0b6468b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b3489-942c-4aa7-bfe7-224b73f45ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
