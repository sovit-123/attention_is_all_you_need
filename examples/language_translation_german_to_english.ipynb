{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d1c5b81-10d0-4d45-bda5-681651cf547d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook uses the transformer neural network from this repository (custom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f5c975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 22:27:31.187852: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-12 22:27:31.661063: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-11.4/lib64:\n",
      "2023-07-12 22:27:31.661171: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-11.4/lib64:\n",
      "2023-07-12 22:27:31.661179: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-07-12 22:27:32.270013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-12 22:27:32.270458: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-12 22:27:32.270720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: setuptools in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (63.4.1)\n",
      "Requirement already satisfied: jinja2 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.30.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.2.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.2)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "2023-07-12 22:27:36.037611: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-12 22:27:36.532894: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-11.4/lib64:\n",
      "2023-07-12 22:27:36.533024: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-11.4/lib64:\n",
      "2023-07-12 22:27:36.533031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 22:27:37.132990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-12 22:27:37.133464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-12 22:27:37.133749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "Collecting de-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.5.0/de_core_news_sm-3.5.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from de-core-news-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.23.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: jinja2 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.3)\n",
      "Requirement already satisfied: setuptools in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (63.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (6.2.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.30.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8feed167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from timeit import default_timer as timer\n",
    "from attention import transformer\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "657b42d4-4062-4415-95d5-bdef7956c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c92e25-0c84-47b4-8733-c75d9e95a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "042737e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 20:07:38.761737: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-27 20:07:39.510266: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-06-27 20:07:40.405306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-27 20:07:40.405673: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "    \n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(\n",
    "        yield_tokens(train_iter, ln),\n",
    "        min_freq=1,\n",
    "        specials=special_symbols,\n",
    "        special_first=True,\n",
    "    )\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67892e63",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bf0d1bf-6545-432e-893a-bb0341f1d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "MAX_LEN = 256\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "DEVICE = 'cuda'\n",
    "NUM_EPOCHS = 75\n",
    "# DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd94f9",
   "metadata": {},
   "source": [
    "## Utilites for Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c332bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b20784",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7bc121b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36,035,349 total parameters.\n",
      "36,035,349 training parameters.\n",
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embedding): Embedding(\n",
      "      (embed): Embedding(19214, 512)\n",
      "    )\n",
      "    (positional_encoding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embedding): Embedding(\n",
      "      (embed): Embedding(10837, 512)\n",
      "    )\n",
      "    (postional_encoding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (ffn): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (ffn): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (ffn): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Linear(in_features=512, out_features=10837, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = transformer.Transformer(\n",
    "    embed_dim=EMB_SIZE,\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    seq_len=MAX_LEN,\n",
    "    num_layers=NUM_ENCODER_LAYERS,\n",
    "    n_heads=NHEAD,\n",
    "    device=DEVICE,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c25777a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.parameters():\n",
    "#     if p.dim() > 1:\n",
    "#         nn.init.xavier_uniform_(p)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a41f2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        # print(\" \".join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        # print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        logits = model(src, tgt_input)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.view(-1, TGT_VOCAB_SIZE), tgt_out.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "\n",
    "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        # print(\" \".join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        # print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]\n",
    "        \n",
    "        logits = model(src, tgt_input)\n",
    "\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.view(-1, TGT_VOCAB_SIZE), tgt_out.contiguous().view(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7efaf99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 5.660, Val loss: 4.819, Epoch time = 15.202s\n",
      "Epoch: 2, Train loss: 4.551, Val loss: 4.214, Epoch time = 14.745s\n",
      "Epoch: 3, Train loss: 4.086, Val loss: 3.845, Epoch time = 14.329s\n",
      "Epoch: 4, Train loss: 3.777, Val loss: 3.568, Epoch time = 14.360s\n",
      "Epoch: 5, Train loss: 3.539, Val loss: 3.369, Epoch time = 14.334s\n",
      "Epoch: 6, Train loss: 3.351, Val loss: 3.211, Epoch time = 14.339s\n",
      "Epoch: 7, Train loss: 3.200, Val loss: 3.091, Epoch time = 14.351s\n",
      "Epoch: 8, Train loss: 3.072, Val loss: 2.989, Epoch time = 14.539s\n",
      "Epoch: 9, Train loss: 2.959, Val loss: 2.901, Epoch time = 15.555s\n",
      "Epoch: 10, Train loss: 2.860, Val loss: 2.819, Epoch time = 14.913s\n",
      "Epoch: 11, Train loss: 2.773, Val loss: 2.754, Epoch time = 14.897s\n",
      "Epoch: 12, Train loss: 2.693, Val loss: 2.685, Epoch time = 14.950s\n",
      "Epoch: 13, Train loss: 2.619, Val loss: 2.639, Epoch time = 15.012s\n",
      "Epoch: 14, Train loss: 2.555, Val loss: 2.584, Epoch time = 14.942s\n",
      "Epoch: 15, Train loss: 2.489, Val loss: 2.533, Epoch time = 15.346s\n",
      "Epoch: 16, Train loss: 2.434, Val loss: 2.492, Epoch time = 15.290s\n",
      "Epoch: 17, Train loss: 2.380, Val loss: 2.455, Epoch time = 15.274s\n",
      "Epoch: 18, Train loss: 2.330, Val loss: 2.425, Epoch time = 15.447s\n",
      "Epoch: 19, Train loss: 2.279, Val loss: 2.389, Epoch time = 15.328s\n",
      "Epoch: 20, Train loss: 2.236, Val loss: 2.364, Epoch time = 14.979s\n",
      "Epoch: 21, Train loss: 2.195, Val loss: 2.329, Epoch time = 15.156s\n",
      "Epoch: 22, Train loss: 2.153, Val loss: 2.302, Epoch time = 14.573s\n",
      "Epoch: 23, Train loss: 2.115, Val loss: 2.286, Epoch time = 14.332s\n",
      "Epoch: 24, Train loss: 2.076, Val loss: 2.260, Epoch time = 14.603s\n",
      "Epoch: 25, Train loss: 2.040, Val loss: 2.256, Epoch time = 14.489s\n",
      "Epoch: 26, Train loss: 2.006, Val loss: 2.233, Epoch time = 14.586s\n",
      "Epoch: 27, Train loss: 1.971, Val loss: 2.214, Epoch time = 14.559s\n",
      "Epoch: 28, Train loss: 1.941, Val loss: 2.205, Epoch time = 15.187s\n",
      "Epoch: 29, Train loss: 1.908, Val loss: 2.186, Epoch time = 15.342s\n",
      "Epoch: 30, Train loss: 1.879, Val loss: 2.182, Epoch time = 16.565s\n",
      "Epoch: 31, Train loss: 1.849, Val loss: 2.151, Epoch time = 16.831s\n",
      "Epoch: 32, Train loss: 1.822, Val loss: 2.139, Epoch time = 14.448s\n",
      "Epoch: 33, Train loss: 1.795, Val loss: 2.131, Epoch time = 15.014s\n",
      "Epoch: 34, Train loss: 1.770, Val loss: 2.125, Epoch time = 15.640s\n",
      "Epoch: 35, Train loss: 1.741, Val loss: 2.098, Epoch time = 14.878s\n",
      "Epoch: 36, Train loss: 1.717, Val loss: 2.093, Epoch time = 14.302s\n",
      "Epoch: 37, Train loss: 1.691, Val loss: 2.077, Epoch time = 15.060s\n",
      "Epoch: 38, Train loss: 1.670, Val loss: 2.072, Epoch time = 14.965s\n",
      "Epoch: 39, Train loss: 1.645, Val loss: 2.058, Epoch time = 14.556s\n",
      "Epoch: 40, Train loss: 1.624, Val loss: 2.051, Epoch time = 14.820s\n",
      "Epoch: 41, Train loss: 1.602, Val loss: 2.040, Epoch time = 14.810s\n",
      "Epoch: 42, Train loss: 1.579, Val loss: 2.044, Epoch time = 14.625s\n",
      "Epoch: 43, Train loss: 1.557, Val loss: 2.031, Epoch time = 14.864s\n",
      "Epoch: 44, Train loss: 1.537, Val loss: 2.028, Epoch time = 14.842s\n",
      "Epoch: 45, Train loss: 1.518, Val loss: 2.021, Epoch time = 14.387s\n",
      "Epoch: 46, Train loss: 1.499, Val loss: 2.021, Epoch time = 14.823s\n",
      "Epoch: 47, Train loss: 1.478, Val loss: 2.010, Epoch time = 14.966s\n",
      "Epoch: 48, Train loss: 1.461, Val loss: 2.005, Epoch time = 14.814s\n",
      "Epoch: 49, Train loss: 1.442, Val loss: 1.999, Epoch time = 15.011s\n",
      "Epoch: 50, Train loss: 1.422, Val loss: 1.997, Epoch time = 14.610s\n",
      "Epoch: 51, Train loss: 1.407, Val loss: 2.000, Epoch time = 14.421s\n",
      "Epoch: 52, Train loss: 1.391, Val loss: 1.991, Epoch time = 14.556s\n",
      "Epoch: 53, Train loss: 1.372, Val loss: 1.989, Epoch time = 14.533s\n",
      "Epoch: 54, Train loss: 1.355, Val loss: 1.974, Epoch time = 14.753s\n",
      "Epoch: 55, Train loss: 1.339, Val loss: 1.978, Epoch time = 14.671s\n",
      "Epoch: 56, Train loss: 1.326, Val loss: 1.975, Epoch time = 15.117s\n",
      "Epoch: 57, Train loss: 1.310, Val loss: 1.962, Epoch time = 14.220s\n",
      "Epoch: 58, Train loss: 1.294, Val loss: 1.968, Epoch time = 14.317s\n",
      "Epoch: 59, Train loss: 1.280, Val loss: 1.969, Epoch time = 14.178s\n",
      "Epoch: 60, Train loss: 1.264, Val loss: 1.958, Epoch time = 14.287s\n",
      "Epoch: 61, Train loss: 1.252, Val loss: 1.953, Epoch time = 15.162s\n",
      "Epoch: 62, Train loss: 1.237, Val loss: 1.953, Epoch time = 15.990s\n",
      "Epoch: 63, Train loss: 1.223, Val loss: 1.952, Epoch time = 14.978s\n",
      "Epoch: 64, Train loss: 1.207, Val loss: 1.955, Epoch time = 14.737s\n",
      "Epoch: 65, Train loss: 1.196, Val loss: 1.948, Epoch time = 16.071s\n",
      "Epoch: 66, Train loss: 1.185, Val loss: 1.953, Epoch time = 15.892s\n",
      "Epoch: 67, Train loss: 1.169, Val loss: 1.948, Epoch time = 15.695s\n",
      "Epoch: 68, Train loss: 1.156, Val loss: 1.953, Epoch time = 15.505s\n",
      "Epoch: 69, Train loss: 1.143, Val loss: 1.949, Epoch time = 15.500s\n",
      "Epoch: 70, Train loss: 1.133, Val loss: 1.955, Epoch time = 15.428s\n",
      "Epoch: 71, Train loss: 1.119, Val loss: 1.954, Epoch time = 15.606s\n",
      "Epoch: 72, Train loss: 1.109, Val loss: 1.951, Epoch time = 14.793s\n",
      "Epoch: 73, Train loss: 1.096, Val loss: 1.946, Epoch time = 14.426s\n",
      "Epoch: 74, Train loss: 1.086, Val loss: 1.955, Epoch time = 14.418s\n",
      "Epoch: 75, Train loss: 1.074, Val loss: 1.960, Epoch time = 14.411s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88e5fc54-9ff3-4df2-8c15-c1f257fe7408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('outputs/translation', exist_ok=True)\n",
    "torch.save(model, 'outputs/translation/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317dbf60-1a73-4de5-abc1-b92d0f750f40",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da73a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4651a7e9-9cdf-4a0c-b89b-3d9cf8fa9d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from attention.transformer import TransformerDecoder, TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82069238-8a62-4b4d-a2d3-196e196db258",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('outputs/translation/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9a722e9-879e-4c94-91ee-230b6bcc54a7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embedding): Embedding(\n",
      "      (embed): Embedding(19214, 512)\n",
      "    )\n",
      "    (positional_encoding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embedding): Embedding(\n",
      "      (embed): Embedding(10837, 512)\n",
      "    )\n",
      "    (postional_encoding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (ffn): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (ffn): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (ffn): Sequential(\n",
      "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Linear(in_features=512, out_features=10837, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "635dc734-e284-428a-89ee-9a33505d49e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tgt_mask(tgt, pad_token_id=1):\n",
    "    \"\"\"\n",
    "    :param tgt: Target sequence.\n",
    "    Returns:\n",
    "        tgt_mask: Target mask.\n",
    "    \"\"\"\n",
    "    batch_size = tgt.shape[0]\n",
    "    device = tgt.device\n",
    "\n",
    "    # Same as src_mask but we additionally want to mask tokens from looking forward into the future tokens\n",
    "    # Note: wherever the mask value is true we want to attend to that token, otherwise we mask (ignore) it.\n",
    "    sequence_length = tgt.shape[1]  # trg_token_ids shape = (B, T) where T max trg token-sequence length\n",
    "    trg_padding_mask = (tgt != pad_token_id).view(batch_size, 1, 1, -1)  # shape = (B, 1, 1, T)\n",
    "    trg_no_look_forward_mask = torch.triu(torch.ones((1, 1, sequence_length, sequence_length), device=device) == 1).transpose(2, 3)\n",
    "\n",
    "    # logic AND operation (both padding mask and no-look-forward must be true to attend to a certain target token)\n",
    "    tgt_mask = trg_padding_mask & trg_no_look_forward_mask  # final shape = (B, 1, T, T)\n",
    "    return tgt_mask\n",
    "    \n",
    "def make_src_mask(src, pad_token_id=1):\n",
    "    \"\"\"\n",
    "    :param src: Source sequence.\n",
    "\n",
    "    Returns:\n",
    "        src_mask: Source mask.\n",
    "    \"\"\"\n",
    "    batch_size = src.shape[0]\n",
    "\n",
    "    # src_mask shape = (B, 1, 1, S) check out attention function in transformer_model.py where masks are applied\n",
    "    # src_mask only masks pad tokens as we want to ignore their representations (no information in there...)\n",
    "    src_mask = (src != pad_token_id).view(batch_size, 1, 1, -1)\n",
    "    return src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5197023d-7bf1-47bb-a53c-c05813c3d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransformerDecoder(\n",
    "            TGT_VOCAB_SIZE,\n",
    "            EMB_SIZE,\n",
    "            MAX_LEN,\n",
    "            NUM_ENCODER_LAYERS,\n",
    "            expansion_factor=4,\n",
    "            n_heads=NHEAD\n",
    "        ).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f1d6df0-244d-4eb4-980c-1afeb5ec0f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.load_state_dict(model.decoder.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d807ba81-35e0-4d6e-be10-49dff824527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(\n",
    "            MAX_LEN,\n",
    "            SRC_VOCAB_SIZE,\n",
    "            EMB_SIZE,\n",
    "            NUM_ENCODER_LAYERS,\n",
    "            expansion_factor=4,\n",
    "            n_heads=NHEAD\n",
    "        ).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e06a966-864d-409e-b665-34ed890acfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(model.encoder.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe75f44f-97d7-4aac-8552-b47f61392445",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embedding): Embedding(\n",
       "      (embed): Embedding(19214, 512)\n",
       "    )\n",
       "    (positional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (embedding): Embedding(\n",
       "      (embed): Embedding(10837, 512)\n",
       "    )\n",
       "    (postional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (transformer_block): TransformerBlock(\n",
       "          (attention): MultiHeadAttention(\n",
       "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (v): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (transformer_block): TransformerBlock(\n",
       "          (attention): MultiHeadAttention(\n",
       "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (v): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (transformer_block): TransformerBlock(\n",
       "          (attention): MultiHeadAttention(\n",
       "            (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (k): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (v): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=512, out_features=10837, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0db54237-1e21-47ec-b4f1-13ac8522a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(src, tgt):\n",
    "    \"\"\"\n",
    "    :param src: Encoder input\n",
    "    :param tgt: Decoder input\n",
    "\n",
    "    Returns:\n",
    "        out_labels: Final prediction sequence\n",
    "    \"\"\"\n",
    "    tgt_mask = make_tgt_mask(tgt).to(DEVICE)\n",
    "    src_mask = make_src_mask(src).to(DEVICE)\n",
    "    enc_out = encoder(src)\n",
    "    out_labels = []\n",
    "    batch_size, seq_len = src.shape[0], src.shape[1]\n",
    "    out = tgt\n",
    "    with torch.no_grad():\n",
    "        for i in range(seq_len):\n",
    "            if i != 0:\n",
    "                tgt = torch.tensor(out_labels, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
    "                # print(tgt)\n",
    "                out = decoder(torch.tensor(tgt).to(DEVICE), enc_out, src_mask, tgt_mask)\n",
    "            else:\n",
    "                out = decoder(out, enc_out, src_mask, tgt_mask)\n",
    "            out = out.reshape(-1, out.shape[-1])\n",
    "            num_of_trg_tokens = len(tgt[0])\n",
    "            out = out[num_of_trg_tokens-1::num_of_trg_tokens]\n",
    "            out = torch.argmax(out, dim=-1)\n",
    "            out_labels.append(out.item())\n",
    "            out = torch.unsqueeze(out, 0)\n",
    "        return out_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17ace8-a2ad-4ebd-a1b0-0d982612b98b",
   "metadata": {},
   "source": [
    "### Some Test Samples\n",
    "\n",
    "Top - English\\\n",
    "Bottom - German\n",
    "\n",
    "A man in an orange hat starring at something.\\\n",
    "Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.\n",
    "\n",
    "A Boston Terrier is running on lush green grass in front of a white fence.\\\n",
    "Ein Boston Terrier läuft über saftig-grünes Gras vor einem weißen Zaun.\n",
    "\n",
    "A girl in karate uniform breaking a stick with a front kick.\\\n",
    "Ein Mädchen in einem Karateanzug bricht einen Stock mit einem Tritt.\n",
    "\n",
    "Five people wearing winter jackets and helmets stand in the snow, with snowmobiles in the background...\\\n",
    "Fünf Leute in Winterjacken und mit Helmen stehen im Schnee mit Schneemobilen im Hintergrund.\n",
    "\n",
    "People are fixing the roof of a house.\\\n",
    "Leute Reparieren das Dach eines Hauses.\n",
    "\n",
    "A man in light colored clothing photographs a group of men wearing dark suits and hats standing arou...\\\n",
    "Ein hell gekleideter Mann fotografiert eine Gruppe von Männern in dunklen Anzügen und mit Hüten, die...\n",
    "\n",
    "A group of people standing in front of an igloo.\\\n",
    "Eine Gruppe von Menschen steht vor einem Iglu.\n",
    "\n",
    "A boy in a red uniform is attempting to avoid getting out at home plate, while the catcher in the bl...\\\n",
    "Ein Junge in einem roten Trikot versucht, die Home Base zu erreichen, während der Catcher im blauen ...\n",
    "\n",
    "A guy works on a building.\\\n",
    "Ein Typ arbeitet an einem Gebäude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c060e91-035d-48f8-a6bc-dee1300960f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five in winter jackets and helmets are in the winter hats with snow in the background .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_241552/2588931828.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = decoder(torch.tensor(tgt).to(DEVICE), enc_out, src_mask, tgt_mask)\n"
     ]
    }
   ],
   "source": [
    "# Full-stops are important for the model to perform well.\n",
    "src_sentence = \"Fünf Leute in Winterjacken und mit Helmen stehen im Schnee mit Schneemobilen im Hintergrund.\"\n",
    "start_symbol = BOS_IDX\n",
    "src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "num_tokens = src.shape[0]\n",
    "src = src.to(DEVICE)\n",
    "ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "out = decode(torch.ravel(src).unsqueeze(0), ys)\n",
    "print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(out))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaec6c2-b154-47fb-bcab-74d0b6468b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b3489-942c-4aa7-bfe7-224b73f45ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
