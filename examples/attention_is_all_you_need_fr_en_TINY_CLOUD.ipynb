{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4dd09e0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.010303,
     "end_time": "2024-01-16T13:46:05.213230",
     "exception": false,
     "start_time": "2024-01-16T13:46:05.202927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Find small single text files for **Language Modeling** experiements here ⬇️\n",
    "\n",
    "https://www.kaggle.com/datasets/devicharith/language-translation-englishfrench\n",
    "\n",
    "This is an end-to-end runnable notebook that clones the repository and starts the training. Ideal for uploading on cloud machines and start training. This notebooks uses a much smaller verision of the original Transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f882c1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:46:05.233810Z",
     "iopub.status.busy": "2024-01-16T13:46:05.233451Z",
     "iopub.status.idle": "2024-01-16T13:46:23.971983Z",
     "shell.execute_reply": "2024-01-16T13:46:23.970801Z"
    },
    "papermill": {
     "duration": 18.751452,
     "end_time": "2024-01-16T13:46:23.974395",
     "exception": false,
     "start_time": "2024-01-16T13:46:05.222943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'attention_is_all_you_need'...\r\n",
      "remote: Enumerating objects: 334, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (52/52), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (37/37), done.\u001b[K\r\n",
      "remote: Total 334 (delta 24), reused 37 (delta 15), pack-reused 282\u001b[K\r\n",
      "Receiving objects: 100% (334/334), 7.32 MiB | 12.82 MiB/s, done.\r\n",
      "Resolving deltas: 100% (199/199), done.\r\n",
      "/kaggle/working/attention_is_all_you_need\n",
      "Branch 'pre_norm' set up to track remote branch 'pre_norm' from 'origin'.\r\n",
      "Switched to a new branch 'pre_norm'\r\n",
      "Processing /kaggle/working/attention_is_all_you_need\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: attention\r\n",
      "  Building wheel for attention (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for attention: filename=attention-1.0-py3-none-any.whl size=7505 sha256=c8b6b094450e7dc6942501caa0918e33c0484e6d16165ce6404fecabaca042fb\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-w3u_zocf/wheels/21/f5/d1/4f7cca7147429101613f6ad272b60b3e6c4e8c48c87f4ccdd1\r\n",
      "Successfully built attention\r\n",
      "Installing collected packages: attention\r\n",
      "Successfully installed attention-1.0\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sovit-123/attention_is_all_you_need.git\n",
    "%cd attention_is_all_you_need\n",
    "!git checkout pre_norm\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1dd16d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:46:23.999783Z",
     "iopub.status.busy": "2024-01-16T13:46:23.999429Z",
     "iopub.status.idle": "2024-01-16T13:47:23.471485Z",
     "shell.execute_reply": "2024-01-16T13:47:23.470376Z"
    },
    "papermill": {
     "duration": 59.487583,
     "end_time": "2024-01-16T13:47:23.473897",
     "exception": false,
     "start_time": "2024-01-16T13:46:23.986314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting install\r\n",
      "  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\r\n",
      "Collecting portalocker\r\n",
      "  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl.metadata\r\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\r\n",
      "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\r\n",
      "Installing collected packages: portalocker, install\r\n",
      "Successfully installed install-1.3.5 portalocker-2.8.2\r\n",
      "Requirement already satisfied: torchtext in /opt/conda/lib/python3.10/site-packages (0.15.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext) (4.66.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchtext) (2.31.0)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchtext) (2.0.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext) (1.24.3)\r\n",
      "Requirement already satisfied: torchdata in /opt/conda/lib/python3.10/site-packages (from torchtext) (0.7.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (2023.11.17)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchtext) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchtext) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchtext) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchtext) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchtext) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchtext) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchtext) (1.3.0)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.24.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (3.7.2)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy) (8.2.2)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.3.4)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.9.0)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (6.3.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (4.66.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.31.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.10.12)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (68.1.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (21.3)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.24.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.9)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.10)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U install portalocker\n",
    "!pip install torchtext\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878e5eb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:47:23.502994Z",
     "iopub.status.busy": "2024-01-16T13:47:23.502642Z",
     "iopub.status.idle": "2024-01-16T13:48:02.912128Z",
     "shell.execute_reply": "2024-01-16T13:48:02.911115Z"
    },
    "papermill": {
     "duration": 39.427237,
     "end_time": "2024-01-16T13:48:02.914557",
     "exception": false,
     "start_time": "2024-01-16T13:47:23.487320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "Collecting en-core-web-sm==3.7.1\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.3.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.12)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.1.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.5.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.10)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\r\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "Collecting fr-core-news-sm==3.7.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from fr-core-news-sm==3.7.0) (3.7.2)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.2)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.3.4)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.9.0)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (6.3.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.31.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.10.12)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (68.1.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (21.3)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.24.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.5.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2023.11.17)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.10)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.4)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.1.3)\r\n",
      "Installing collected packages: fr-core-news-sm\r\n",
      "Successfully installed fr-core-news-sm-3.7.0\r\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def51ed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:02.949628Z",
     "iopub.status.busy": "2024-01-16T13:48:02.949011Z",
     "iopub.status.idle": "2024-01-16T13:48:02.955665Z",
     "shell.execute_reply": "2024-01-16T13:48:02.954614Z"
    },
    "papermill": {
     "duration": 0.026483,
     "end_time": "2024-01-16T13:48:02.957698",
     "exception": false,
     "start_time": "2024-01-16T13:48:02.931215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/attention_is_all_you_need/examples\n"
     ]
    }
   ],
   "source": [
    "%cd examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e89b4417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:02.991535Z",
     "iopub.status.busy": "2024-01-16T13:48:02.990875Z",
     "iopub.status.idle": "2024-01-16T13:48:07.292614Z",
     "shell.execute_reply": "2024-01-16T13:48:07.291695Z"
    },
    "papermill": {
     "duration": 4.320994,
     "end_time": "2024-01-16T13:48:07.294975",
     "exception": false,
     "start_time": "2024-01-16T13:48:02.973981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from timeit import default_timer as timer\n",
    "from attention import transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1b2c9d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:07.330109Z",
     "iopub.status.busy": "2024-01-16T13:48:07.329580Z",
     "iopub.status.idle": "2024-01-16T13:48:07.338607Z",
     "shell.execute_reply": "2024-01-16T13:48:07.337924Z"
    },
    "papermill": {
     "duration": 0.02839,
     "end_time": "2024-01-16T13:48:07.340543",
     "exception": false,
     "start_time": "2024-01-16T13:48:07.312153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set seed.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7a1d4b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:07.374794Z",
     "iopub.status.busy": "2024-01-16T13:48:07.374497Z",
     "iopub.status.idle": "2024-01-16T13:48:07.378600Z",
     "shell.execute_reply": "2024-01-16T13:48:07.377772Z"
    },
    "papermill": {
     "duration": 0.023174,
     "end_time": "2024-01-16T13:48:07.380473",
     "exception": false,
     "start_time": "2024-01-16T13:48:07.357299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'fr'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67162b9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:07.414060Z",
     "iopub.status.busy": "2024-01-16T13:48:07.413800Z",
     "iopub.status.idle": "2024-01-16T13:48:17.336889Z",
     "shell.execute_reply": "2024-01-16T13:48:17.336081Z"
    },
    "papermill": {
     "duration": 9.942638,
     "end_time": "2024-01-16T13:48:17.339252",
     "exception": false,
     "start_time": "2024-01-16T13:48:07.396614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='fr_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3c63f36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:17.424224Z",
     "iopub.status.busy": "2024-01-16T13:48:17.423212Z",
     "iopub.status.idle": "2024-01-16T13:48:17.751445Z",
     "shell.execute_reply": "2024-01-16T13:48:17.750495Z"
    },
    "papermill": {
     "duration": 0.399195,
     "end_time": "2024-01-16T13:48:17.755248",
     "exception": false,
     "start_time": "2024-01-16T13:48:17.356053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English words/sentences French words/sentences\n",
       "0                     Hi.                 Salut!\n",
       "1                    Run!                Cours !\n",
       "2                    Run!               Courez !\n",
       "3                    Who?                  Qui ?\n",
       "4                    Wow!             Ça alors !"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = pd.read_csv(\n",
    "    'data/english_french/eng_-french.csv', \n",
    "    usecols=['English words/sentences', 'French words/sentences']\n",
    ")\n",
    "csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ab556a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:17.790848Z",
     "iopub.status.busy": "2024-01-16T13:48:17.790478Z",
     "iopub.status.idle": "2024-01-16T13:48:17.820658Z",
     "shell.execute_reply": "2024-01-16T13:48:17.819770Z"
    },
    "papermill": {
     "duration": 0.050306,
     "end_time": "2024-01-16T13:48:17.822658",
     "exception": false,
     "start_time": "2024-01-16T13:48:17.772352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_csv, test_csv = train_test_split(csv, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ab362d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:17.857274Z",
     "iopub.status.busy": "2024-01-16T13:48:17.856952Z",
     "iopub.status.idle": "2024-01-16T13:48:17.861515Z",
     "shell.execute_reply": "2024-01-16T13:48:17.860573Z"
    },
    "papermill": {
     "duration": 0.02425,
     "end_time": "2024-01-16T13:48:17.863649",
     "exception": false,
     "start_time": "2024-01-16T13:48:17.839399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158058\n",
      "17563\n"
     ]
    }
   ],
   "source": [
    "print(len(train_csv))\n",
    "print(len(test_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13ddc6fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:17.898215Z",
     "iopub.status.busy": "2024-01-16T13:48:17.897935Z",
     "iopub.status.idle": "2024-01-16T13:48:17.906884Z",
     "shell.execute_reply": "2024-01-16T13:48:17.905953Z"
    },
    "papermill": {
     "duration": 0.028254,
     "end_time": "2024-01-16T13:48:17.908798",
     "exception": false,
     "start_time": "2024-01-16T13:48:17.880544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158383</th>\n",
       "      <td>They kept him waiting outside for a long time.</td>\n",
       "      <td>Ils le firent poireauter dehors.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146722</th>\n",
       "      <td>How much money did you spend on your car?</td>\n",
       "      <td>Combien d'argent avez-vous dépensé pour votre ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120085</th>\n",
       "      <td>I heard it from a reliable source.</td>\n",
       "      <td>Je l'ai entendu d'une source fiable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152460</th>\n",
       "      <td>My parents met each other in the mountains.</td>\n",
       "      <td>Mes parents se sont rencontrés dans les montag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63136</th>\n",
       "      <td>My teacher drove me home.</td>\n",
       "      <td>Mon professeur m'a reconduit chez moi.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               English words/sentences  \\\n",
       "158383  They kept him waiting outside for a long time.   \n",
       "146722       How much money did you spend on your car?   \n",
       "120085              I heard it from a reliable source.   \n",
       "152460     My parents met each other in the mountains.   \n",
       "63136                        My teacher drove me home.   \n",
       "\n",
       "                                   French words/sentences  \n",
       "158383                   Ils le firent poireauter dehors.  \n",
       "146722  Combien d'argent avez-vous dépensé pour votre ...  \n",
       "120085               Je l'ai entendu d'une source fiable.  \n",
       "152460  Mes parents se sont rencontrés dans les montag...  \n",
       "63136              Mon professeur m'a reconduit chez moi.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d052560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:17.943841Z",
     "iopub.status.busy": "2024-01-16T13:48:17.943228Z",
     "iopub.status.idle": "2024-01-16T13:48:17.951782Z",
     "shell.execute_reply": "2024-01-16T13:48:17.950764Z"
    },
    "papermill": {
     "duration": 0.028389,
     "end_time": "2024-01-16T13:48:17.953912",
     "exception": false,
     "start_time": "2024-01-16T13:48:17.925523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2785</th>\n",
       "      <td>Take a seat.</td>\n",
       "      <td>Prends place !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29880</th>\n",
       "      <td>I wish Tom was here.</td>\n",
       "      <td>J'aimerais que Tom soit là.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53776</th>\n",
       "      <td>How did the audition go?</td>\n",
       "      <td>Comment s'est passée l'audition ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154386</th>\n",
       "      <td>I've no friend to talk to about my problems.</td>\n",
       "      <td>Je n'ai pas d'ami avec lequel je puisse m'entr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149823</th>\n",
       "      <td>I really like this skirt. Can I try it on?</td>\n",
       "      <td>J'aime beaucoup cette jupe, puis-je l'essayer ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             English words/sentences  \\\n",
       "2785                                    Take a seat.   \n",
       "29880                           I wish Tom was here.   \n",
       "53776                       How did the audition go?   \n",
       "154386  I've no friend to talk to about my problems.   \n",
       "149823    I really like this skirt. Can I try it on?   \n",
       "\n",
       "                                   French words/sentences  \n",
       "2785                                       Prends place !  \n",
       "29880                         J'aimerais que Tom soit là.  \n",
       "53776                   Comment s'est passée l'audition ?  \n",
       "154386  Je n'ai pas d'ami avec lequel je puisse m'entr...  \n",
       "149823    J'aime beaucoup cette jupe, puis-je l'essayer ?  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3b1d3f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:17.990443Z",
     "iopub.status.busy": "2024-01-16T13:48:17.989870Z",
     "iopub.status.idle": "2024-01-16T13:48:17.995130Z",
     "shell.execute_reply": "2024-01-16T13:48:17.994210Z"
    },
    "papermill": {
     "duration": 0.025177,
     "end_time": "2024-01-16T13:48:17.997071",
     "exception": false,
     "start_time": "2024-01-16T13:48:17.971894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ils le firent poireauter dehors.\n",
      "They kept him waiting outside for a long time.\n"
     ]
    }
   ],
   "source": [
    "print(train_csv['French words/sentences'].iloc[0])\n",
    "print(train_csv['English words/sentences'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5836a76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:18.032430Z",
     "iopub.status.busy": "2024-01-16T13:48:18.032132Z",
     "iopub.status.idle": "2024-01-16T13:48:18.037326Z",
     "shell.execute_reply": "2024-01-16T13:48:18.036522Z"
    },
    "papermill": {
     "duration": 0.025055,
     "end_time": "2024-01-16T13:48:18.039142",
     "exception": false,
     "start_time": "2024-01-16T13:48:18.014087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, csv):\n",
    "        self.csv = csv\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return(\n",
    "            self.csv['French words/sentences'].iloc[idx],\n",
    "            self.csv['English words/sentences'].iloc[idx]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "214659e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:18.074464Z",
     "iopub.status.busy": "2024-01-16T13:48:18.074183Z",
     "iopub.status.idle": "2024-01-16T13:48:18.078001Z",
     "shell.execute_reply": "2024-01-16T13:48:18.077175Z"
    },
    "papermill": {
     "duration": 0.023688,
     "end_time": "2024-01-16T13:48:18.079854",
     "exception": false,
     "start_time": "2024-01-16T13:48:18.056166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(train_csv)\n",
    "valid_dataset = TranslationDataset(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b697a4d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:18.115072Z",
     "iopub.status.busy": "2024-01-16T13:48:18.114806Z",
     "iopub.status.idle": "2024-01-16T13:48:18.119136Z",
     "shell.execute_reply": "2024-01-16T13:48:18.118287Z"
    },
    "papermill": {
     "duration": 0.024349,
     "end_time": "2024-01-16T13:48:18.121228",
     "exception": false,
     "start_time": "2024-01-16T13:48:18.096879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ils le firent poireauter dehors.', 'They kept him waiting outside for a long time.')\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(train_dataset)\n",
    "print(next(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e95cefb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:18.156793Z",
     "iopub.status.busy": "2024-01-16T13:48:18.156499Z",
     "iopub.status.idle": "2024-01-16T13:48:43.429605Z",
     "shell.execute_reply": "2024-01-16T13:48:43.428785Z"
    },
    "papermill": {
     "duration": 25.29349,
     "end_time": "2024-01-16T13:48:43.431912",
     "exception": false,
     "start_time": "2024-01-16T13:48:18.138422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(\n",
    "        yield_tokens(train_dataset, ln),\n",
    "        min_freq=1,\n",
    "        specials=special_symbols,\n",
    "        special_first=True,\n",
    "    )\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "455c13b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:43.468378Z",
     "iopub.status.busy": "2024-01-16T13:48:43.468099Z",
     "iopub.status.idle": "2024-01-16T13:48:43.473204Z",
     "shell.execute_reply": "2024-01-16T13:48:43.472480Z"
    },
    "papermill": {
     "duration": 0.025379,
     "end_time": "2024-01-16T13:48:43.475045",
     "exception": false,
     "start_time": "2024-01-16T13:48:43.449666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 256\n",
    "NHEAD = 2\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 512\n",
    "MAX_LEN = 256\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "DEVICE = 'cuda'\n",
    "NUM_EPOCHS = 200\n",
    "DROPOUT = 0.1\n",
    "# DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e8196da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:43.509967Z",
     "iopub.status.busy": "2024-01-16T13:48:43.509671Z",
     "iopub.status.idle": "2024-01-16T13:48:43.518034Z",
     "shell.execute_reply": "2024-01-16T13:48:43.517221Z"
    },
    "papermill": {
     "duration": 0.027876,
     "end_time": "2024-01-16T13:48:43.519861",
     "exception": false,
     "start_time": "2024-01-16T13:48:43.491985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffda3e84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:43.554978Z",
     "iopub.status.busy": "2024-01-16T13:48:43.554678Z",
     "iopub.status.idle": "2024-01-16T13:48:43.948612Z",
     "shell.execute_reply": "2024-01-16T13:48:43.947583Z"
    },
    "papermill": {
     "duration": 0.414629,
     "end_time": "2024-01-16T13:48:43.951506",
     "exception": false,
     "start_time": "2024-01-16T13:48:43.536877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17,176,605 total parameters.\n",
      "17,176,605 training parameters.\n",
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embedding): Embedding(\n",
      "      (embed): Embedding(25319, 256)\n",
      "    )\n",
      "    (positional_encoding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (k): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (v): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embedding): Embedding(\n",
      "      (embed): Embedding(15389, 256)\n",
      "    )\n",
      "    (postional_encoding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (k): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (v): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (k): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (v): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (ffn): Sequential(\n",
      "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Linear(in_features=256, out_features=15389, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = transformer.Transformer(\n",
    "    embed_dim=EMB_SIZE,\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    seq_len=MAX_LEN,\n",
    "    num_layers=NUM_ENCODER_LAYERS,\n",
    "    n_heads=NHEAD,\n",
    "    device=DEVICE,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afb5d550",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:43.987816Z",
     "iopub.status.busy": "2024-01-16T13:48:43.987497Z",
     "iopub.status.idle": "2024-01-16T13:48:43.992783Z",
     "shell.execute_reply": "2024-01-16T13:48:43.991977Z"
    },
    "papermill": {
     "duration": 0.025633,
     "end_time": "2024-01-16T13:48:43.994781",
     "exception": false,
     "start_time": "2024-01-16T13:48:43.969148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "deb3a751",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:44.030622Z",
     "iopub.status.busy": "2024-01-16T13:48:44.030331Z",
     "iopub.status.idle": "2024-01-16T13:48:44.041160Z",
     "shell.execute_reply": "2024-01-16T13:48:44.040293Z"
    },
    "papermill": {
     "duration": 0.031068,
     "end_time": "2024-01-16T13:48:44.043141",
     "exception": false,
     "start_time": "2024-01-16T13:48:44.012073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "#     for src, tgt in tqdm(train_dataloader, total=len(list(train_dataloader))):\n",
    "    for src, tgt in train_dataloader:\n",
    "        # print(\" \".join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        # print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        logits = model(src, tgt_input)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.view(-1, TGT_VOCAB_SIZE), tgt_out.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "\n",
    "val_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "#     for src, tgt in tqdm(val_dataloader, total=len(list(val_dataloader))):\n",
    "    for src, tgt in val_dataloader:\n",
    "        # print(\" \".join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        # print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]\n",
    "        \n",
    "        logits = model(src, tgt_input)\n",
    "\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.view(-1, TGT_VOCAB_SIZE), tgt_out.contiguous().view(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f6749de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:48:44.079193Z",
     "iopub.status.busy": "2024-01-16T13:48:44.078611Z",
     "iopub.status.idle": "2024-01-16T19:28:32.153849Z",
     "shell.execute_reply": "2024-01-16T19:28:32.152897Z"
    },
    "papermill": {
     "duration": 20388.128689,
     "end_time": "2024-01-16T19:28:32.189057",
     "exception": false,
     "start_time": "2024-01-16T13:48:44.060368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 5.221, Val loss: 4.123, Epoch time = 95.262s\n",
      "Epoch: 2, Train loss: 3.869, Val loss: 3.527, Epoch time = 94.060s\n",
      "Epoch: 3, Train loss: 3.440, Val loss: 3.187, Epoch time = 94.182s\n",
      "Epoch: 4, Train loss: 3.155, Val loss: 2.937, Epoch time = 94.173s\n",
      "Epoch: 5, Train loss: 2.934, Val loss: 2.742, Epoch time = 94.708s\n",
      "Epoch: 6, Train loss: 2.753, Val loss: 2.581, Epoch time = 94.172s\n",
      "Epoch: 7, Train loss: 2.600, Val loss: 2.439, Epoch time = 94.447s\n",
      "Epoch: 8, Train loss: 2.469, Val loss: 2.320, Epoch time = 94.226s\n",
      "Epoch: 9, Train loss: 2.354, Val loss: 2.224, Epoch time = 94.056s\n",
      "Epoch: 10, Train loss: 2.256, Val loss: 2.136, Epoch time = 94.714s\n",
      "Epoch: 11, Train loss: 2.167, Val loss: 2.063, Epoch time = 94.657s\n",
      "Epoch: 12, Train loss: 2.091, Val loss: 2.004, Epoch time = 94.165s\n",
      "Epoch: 13, Train loss: 2.021, Val loss: 1.946, Epoch time = 94.138s\n",
      "Epoch: 14, Train loss: 1.959, Val loss: 1.893, Epoch time = 94.118s\n",
      "Epoch: 15, Train loss: 1.902, Val loss: 1.848, Epoch time = 94.961s\n",
      "Epoch: 16, Train loss: 1.851, Val loss: 1.803, Epoch time = 95.410s\n",
      "Epoch: 17, Train loss: 1.801, Val loss: 1.770, Epoch time = 94.989s\n",
      "Epoch: 18, Train loss: 1.759, Val loss: 1.736, Epoch time = 95.058s\n",
      "Epoch: 19, Train loss: 1.718, Val loss: 1.705, Epoch time = 95.341s\n",
      "Epoch: 20, Train loss: 1.680, Val loss: 1.678, Epoch time = 95.060s\n",
      "Epoch: 21, Train loss: 1.643, Val loss: 1.652, Epoch time = 95.540s\n",
      "Epoch: 22, Train loss: 1.611, Val loss: 1.624, Epoch time = 95.047s\n",
      "Epoch: 23, Train loss: 1.579, Val loss: 1.601, Epoch time = 95.112s\n",
      "Epoch: 24, Train loss: 1.550, Val loss: 1.579, Epoch time = 95.063s\n",
      "Epoch: 25, Train loss: 1.521, Val loss: 1.558, Epoch time = 96.937s\n",
      "Epoch: 26, Train loss: 1.497, Val loss: 1.540, Epoch time = 96.812s\n",
      "Epoch: 27, Train loss: 1.473, Val loss: 1.523, Epoch time = 95.812s\n",
      "Epoch: 28, Train loss: 1.447, Val loss: 1.510, Epoch time = 96.213s\n",
      "Epoch: 29, Train loss: 1.428, Val loss: 1.491, Epoch time = 95.719s\n",
      "Epoch: 30, Train loss: 1.404, Val loss: 1.472, Epoch time = 95.567s\n",
      "Epoch: 31, Train loss: 1.384, Val loss: 1.462, Epoch time = 97.519s\n",
      "Epoch: 32, Train loss: 1.366, Val loss: 1.448, Epoch time = 95.545s\n",
      "Epoch: 33, Train loss: 1.346, Val loss: 1.434, Epoch time = 95.418s\n",
      "Epoch: 34, Train loss: 1.329, Val loss: 1.416, Epoch time = 95.422s\n",
      "Epoch: 35, Train loss: 1.312, Val loss: 1.409, Epoch time = 95.588s\n",
      "Epoch: 36, Train loss: 1.295, Val loss: 1.398, Epoch time = 95.354s\n",
      "Epoch: 37, Train loss: 1.281, Val loss: 1.389, Epoch time = 95.552s\n",
      "Epoch: 38, Train loss: 1.265, Val loss: 1.379, Epoch time = 95.367s\n",
      "Epoch: 39, Train loss: 1.252, Val loss: 1.366, Epoch time = 95.419s\n",
      "Epoch: 40, Train loss: 1.236, Val loss: 1.361, Epoch time = 95.586s\n",
      "Epoch: 41, Train loss: 1.224, Val loss: 1.348, Epoch time = 95.888s\n",
      "Epoch: 42, Train loss: 1.210, Val loss: 1.342, Epoch time = 96.154s\n",
      "Epoch: 43, Train loss: 1.199, Val loss: 1.331, Epoch time = 95.875s\n",
      "Epoch: 44, Train loss: 1.187, Val loss: 1.321, Epoch time = 95.973s\n",
      "Epoch: 45, Train loss: 1.173, Val loss: 1.312, Epoch time = 96.066s\n",
      "Epoch: 46, Train loss: 1.164, Val loss: 1.309, Epoch time = 96.067s\n",
      "Epoch: 47, Train loss: 1.152, Val loss: 1.297, Epoch time = 95.352s\n",
      "Epoch: 48, Train loss: 1.141, Val loss: 1.291, Epoch time = 95.043s\n",
      "Epoch: 49, Train loss: 1.132, Val loss: 1.281, Epoch time = 95.294s\n",
      "Epoch: 50, Train loss: 1.122, Val loss: 1.275, Epoch time = 95.289s\n",
      "Epoch: 51, Train loss: 1.111, Val loss: 1.277, Epoch time = 95.120s\n",
      "Epoch: 52, Train loss: 1.102, Val loss: 1.266, Epoch time = 95.796s\n",
      "Epoch: 53, Train loss: 1.094, Val loss: 1.259, Epoch time = 95.246s\n",
      "Epoch: 54, Train loss: 1.084, Val loss: 1.251, Epoch time = 95.238s\n",
      "Epoch: 55, Train loss: 1.076, Val loss: 1.247, Epoch time = 95.453s\n",
      "Epoch: 56, Train loss: 1.068, Val loss: 1.240, Epoch time = 95.607s\n",
      "Epoch: 57, Train loss: 1.060, Val loss: 1.234, Epoch time = 95.859s\n",
      "Epoch: 58, Train loss: 1.052, Val loss: 1.231, Epoch time = 95.698s\n",
      "Epoch: 59, Train loss: 1.043, Val loss: 1.231, Epoch time = 95.525s\n",
      "Epoch: 60, Train loss: 1.036, Val loss: 1.222, Epoch time = 95.837s\n",
      "Epoch: 61, Train loss: 1.029, Val loss: 1.215, Epoch time = 95.554s\n",
      "Epoch: 62, Train loss: 1.022, Val loss: 1.212, Epoch time = 95.770s\n",
      "Epoch: 63, Train loss: 1.015, Val loss: 1.207, Epoch time = 95.694s\n",
      "Epoch: 64, Train loss: 1.008, Val loss: 1.201, Epoch time = 93.955s\n",
      "Epoch: 65, Train loss: 1.001, Val loss: 1.198, Epoch time = 93.667s\n",
      "Epoch: 66, Train loss: 0.994, Val loss: 1.190, Epoch time = 94.169s\n",
      "Epoch: 67, Train loss: 0.987, Val loss: 1.185, Epoch time = 94.067s\n",
      "Epoch: 68, Train loss: 0.981, Val loss: 1.180, Epoch time = 94.469s\n",
      "Epoch: 69, Train loss: 0.975, Val loss: 1.178, Epoch time = 94.187s\n",
      "Epoch: 70, Train loss: 0.969, Val loss: 1.174, Epoch time = 93.919s\n",
      "Epoch: 71, Train loss: 0.964, Val loss: 1.169, Epoch time = 95.211s\n",
      "Epoch: 72, Train loss: 0.956, Val loss: 1.165, Epoch time = 95.288s\n",
      "Epoch: 73, Train loss: 0.952, Val loss: 1.160, Epoch time = 95.356s\n",
      "Epoch: 74, Train loss: 0.945, Val loss: 1.158, Epoch time = 94.492s\n",
      "Epoch: 75, Train loss: 0.940, Val loss: 1.158, Epoch time = 94.611s\n",
      "Epoch: 76, Train loss: 0.935, Val loss: 1.150, Epoch time = 95.205s\n",
      "Epoch: 77, Train loss: 0.928, Val loss: 1.146, Epoch time = 94.805s\n",
      "Epoch: 78, Train loss: 0.924, Val loss: 1.140, Epoch time = 95.032s\n",
      "Epoch: 79, Train loss: 0.919, Val loss: 1.133, Epoch time = 94.634s\n",
      "Epoch: 80, Train loss: 0.914, Val loss: 1.134, Epoch time = 94.526s\n",
      "Epoch: 81, Train loss: 0.910, Val loss: 1.129, Epoch time = 94.613s\n",
      "Epoch: 82, Train loss: 0.904, Val loss: 1.132, Epoch time = 96.419s\n",
      "Epoch: 83, Train loss: 0.900, Val loss: 1.130, Epoch time = 95.843s\n",
      "Epoch: 84, Train loss: 0.895, Val loss: 1.133, Epoch time = 95.114s\n",
      "Epoch: 85, Train loss: 0.890, Val loss: 1.124, Epoch time = 94.977s\n",
      "Epoch: 86, Train loss: 0.886, Val loss: 1.120, Epoch time = 95.053s\n",
      "Epoch: 87, Train loss: 0.882, Val loss: 1.117, Epoch time = 94.009s\n",
      "Epoch: 88, Train loss: 0.876, Val loss: 1.114, Epoch time = 94.204s\n",
      "Epoch: 89, Train loss: 0.873, Val loss: 1.106, Epoch time = 93.707s\n",
      "Epoch: 90, Train loss: 0.868, Val loss: 1.108, Epoch time = 93.844s\n",
      "Epoch: 91, Train loss: 0.863, Val loss: 1.108, Epoch time = 94.072s\n",
      "Epoch: 92, Train loss: 0.860, Val loss: 1.109, Epoch time = 93.851s\n",
      "Epoch: 93, Train loss: 0.855, Val loss: 1.100, Epoch time = 93.893s\n",
      "Epoch: 94, Train loss: 0.851, Val loss: 1.098, Epoch time = 94.152s\n",
      "Epoch: 95, Train loss: 0.847, Val loss: 1.097, Epoch time = 94.001s\n",
      "Epoch: 96, Train loss: 0.844, Val loss: 1.094, Epoch time = 93.914s\n",
      "Epoch: 97, Train loss: 0.839, Val loss: 1.088, Epoch time = 94.421s\n",
      "Epoch: 98, Train loss: 0.836, Val loss: 1.091, Epoch time = 94.200s\n",
      "Epoch: 99, Train loss: 0.833, Val loss: 1.087, Epoch time = 93.865s\n",
      "Epoch: 100, Train loss: 0.828, Val loss: 1.086, Epoch time = 94.065s\n",
      "Epoch: 101, Train loss: 0.827, Val loss: 1.087, Epoch time = 94.039s\n",
      "Epoch: 102, Train loss: 0.823, Val loss: 1.084, Epoch time = 93.898s\n",
      "Epoch: 103, Train loss: 0.818, Val loss: 1.080, Epoch time = 94.416s\n",
      "Epoch: 104, Train loss: 0.817, Val loss: 1.078, Epoch time = 94.108s\n",
      "Epoch: 105, Train loss: 0.811, Val loss: 1.078, Epoch time = 93.753s\n",
      "Epoch: 106, Train loss: 0.809, Val loss: 1.071, Epoch time = 93.720s\n",
      "Epoch: 107, Train loss: 0.805, Val loss: 1.071, Epoch time = 93.927s\n",
      "Epoch: 108, Train loss: 0.801, Val loss: 1.070, Epoch time = 93.822s\n",
      "Epoch: 109, Train loss: 0.797, Val loss: 1.066, Epoch time = 94.188s\n",
      "Epoch: 110, Train loss: 0.795, Val loss: 1.063, Epoch time = 93.863s\n",
      "Epoch: 111, Train loss: 0.791, Val loss: 1.060, Epoch time = 93.689s\n",
      "Epoch: 112, Train loss: 0.788, Val loss: 1.063, Epoch time = 93.420s\n",
      "Epoch: 113, Train loss: 0.786, Val loss: 1.061, Epoch time = 93.462s\n",
      "Epoch: 114, Train loss: 0.783, Val loss: 1.060, Epoch time = 93.555s\n",
      "Epoch: 115, Train loss: 0.779, Val loss: 1.061, Epoch time = 93.351s\n",
      "Epoch: 116, Train loss: 0.776, Val loss: 1.059, Epoch time = 93.370s\n",
      "Epoch: 117, Train loss: 0.773, Val loss: 1.056, Epoch time = 93.257s\n",
      "Epoch: 118, Train loss: 0.770, Val loss: 1.056, Epoch time = 93.151s\n",
      "Epoch: 119, Train loss: 0.767, Val loss: 1.053, Epoch time = 93.357s\n",
      "Epoch: 120, Train loss: 0.765, Val loss: 1.047, Epoch time = 93.172s\n",
      "Epoch: 121, Train loss: 0.762, Val loss: 1.050, Epoch time = 93.266s\n",
      "Epoch: 122, Train loss: 0.758, Val loss: 1.050, Epoch time = 93.388s\n",
      "Epoch: 123, Train loss: 0.754, Val loss: 1.048, Epoch time = 93.213s\n",
      "Epoch: 124, Train loss: 0.754, Val loss: 1.050, Epoch time = 93.146s\n",
      "Epoch: 125, Train loss: 0.752, Val loss: 1.041, Epoch time = 93.569s\n",
      "Epoch: 126, Train loss: 0.748, Val loss: 1.042, Epoch time = 93.329s\n",
      "Epoch: 127, Train loss: 0.745, Val loss: 1.041, Epoch time = 93.191s\n",
      "Epoch: 128, Train loss: 0.743, Val loss: 1.035, Epoch time = 93.244s\n",
      "Epoch: 129, Train loss: 0.740, Val loss: 1.040, Epoch time = 92.853s\n",
      "Epoch: 130, Train loss: 0.737, Val loss: 1.037, Epoch time = 93.135s\n",
      "Epoch: 131, Train loss: 0.735, Val loss: 1.035, Epoch time = 93.153s\n",
      "Epoch: 132, Train loss: 0.733, Val loss: 1.037, Epoch time = 93.218s\n",
      "Epoch: 133, Train loss: 0.730, Val loss: 1.033, Epoch time = 93.358s\n",
      "Epoch: 134, Train loss: 0.726, Val loss: 1.032, Epoch time = 93.490s\n",
      "Epoch: 135, Train loss: 0.725, Val loss: 1.023, Epoch time = 93.675s\n",
      "Epoch: 136, Train loss: 0.723, Val loss: 1.030, Epoch time = 93.237s\n",
      "Epoch: 137, Train loss: 0.720, Val loss: 1.028, Epoch time = 93.292s\n",
      "Epoch: 138, Train loss: 0.719, Val loss: 1.031, Epoch time = 93.141s\n",
      "Epoch: 139, Train loss: 0.716, Val loss: 1.030, Epoch time = 93.196s\n",
      "Epoch: 140, Train loss: 0.713, Val loss: 1.031, Epoch time = 93.554s\n",
      "Epoch: 141, Train loss: 0.712, Val loss: 1.026, Epoch time = 93.292s\n",
      "Epoch: 142, Train loss: 0.709, Val loss: 1.023, Epoch time = 93.295s\n",
      "Epoch: 143, Train loss: 0.707, Val loss: 1.025, Epoch time = 93.207s\n",
      "Epoch: 144, Train loss: 0.704, Val loss: 1.024, Epoch time = 93.230s\n",
      "Epoch: 145, Train loss: 0.703, Val loss: 1.020, Epoch time = 93.339s\n",
      "Epoch: 146, Train loss: 0.699, Val loss: 1.022, Epoch time = 93.171s\n",
      "Epoch: 147, Train loss: 0.697, Val loss: 1.023, Epoch time = 93.100s\n",
      "Epoch: 148, Train loss: 0.695, Val loss: 1.019, Epoch time = 92.976s\n",
      "Epoch: 149, Train loss: 0.693, Val loss: 1.020, Epoch time = 93.276s\n",
      "Epoch: 150, Train loss: 0.692, Val loss: 1.017, Epoch time = 93.241s\n",
      "Epoch: 151, Train loss: 0.689, Val loss: 1.018, Epoch time = 93.515s\n",
      "Epoch: 152, Train loss: 0.688, Val loss: 1.015, Epoch time = 93.325s\n",
      "Epoch: 153, Train loss: 0.686, Val loss: 1.016, Epoch time = 93.170s\n",
      "Epoch: 154, Train loss: 0.684, Val loss: 1.015, Epoch time = 93.223s\n",
      "Epoch: 155, Train loss: 0.681, Val loss: 1.014, Epoch time = 93.314s\n",
      "Epoch: 156, Train loss: 0.680, Val loss: 1.015, Epoch time = 93.442s\n",
      "Epoch: 157, Train loss: 0.677, Val loss: 1.011, Epoch time = 93.270s\n",
      "Epoch: 158, Train loss: 0.676, Val loss: 1.011, Epoch time = 93.530s\n",
      "Epoch: 159, Train loss: 0.673, Val loss: 1.010, Epoch time = 93.332s\n",
      "Epoch: 160, Train loss: 0.671, Val loss: 1.008, Epoch time = 93.061s\n",
      "Epoch: 161, Train loss: 0.669, Val loss: 1.012, Epoch time = 93.395s\n",
      "Epoch: 162, Train loss: 0.668, Val loss: 1.006, Epoch time = 93.418s\n",
      "Epoch: 163, Train loss: 0.666, Val loss: 1.009, Epoch time = 92.988s\n",
      "Epoch: 164, Train loss: 0.665, Val loss: 1.006, Epoch time = 93.160s\n",
      "Epoch: 165, Train loss: 0.662, Val loss: 1.009, Epoch time = 93.251s\n",
      "Epoch: 166, Train loss: 0.661, Val loss: 1.003, Epoch time = 93.402s\n",
      "Epoch: 167, Train loss: 0.658, Val loss: 1.003, Epoch time = 92.891s\n",
      "Epoch: 168, Train loss: 0.657, Val loss: 1.001, Epoch time = 92.875s\n",
      "Epoch: 169, Train loss: 0.655, Val loss: 1.002, Epoch time = 92.950s\n",
      "Epoch: 170, Train loss: 0.654, Val loss: 1.001, Epoch time = 93.138s\n",
      "Epoch: 171, Train loss: 0.652, Val loss: 1.004, Epoch time = 93.116s\n",
      "Epoch: 172, Train loss: 0.650, Val loss: 0.998, Epoch time = 92.898s\n",
      "Epoch: 173, Train loss: 0.648, Val loss: 0.996, Epoch time = 92.960s\n",
      "Epoch: 174, Train loss: 0.647, Val loss: 0.995, Epoch time = 93.091s\n",
      "Epoch: 175, Train loss: 0.645, Val loss: 0.993, Epoch time = 92.970s\n",
      "Epoch: 176, Train loss: 0.643, Val loss: 0.997, Epoch time = 93.006s\n",
      "Epoch: 177, Train loss: 0.641, Val loss: 0.994, Epoch time = 93.071s\n",
      "Epoch: 178, Train loss: 0.640, Val loss: 0.996, Epoch time = 92.987s\n",
      "Epoch: 179, Train loss: 0.638, Val loss: 0.988, Epoch time = 92.930s\n",
      "Epoch: 180, Train loss: 0.637, Val loss: 0.992, Epoch time = 93.063s\n",
      "Epoch: 181, Train loss: 0.634, Val loss: 0.991, Epoch time = 92.997s\n",
      "Epoch: 182, Train loss: 0.634, Val loss: 0.990, Epoch time = 93.274s\n",
      "Epoch: 183, Train loss: 0.631, Val loss: 0.995, Epoch time = 93.046s\n",
      "Epoch: 184, Train loss: 0.629, Val loss: 0.996, Epoch time = 93.046s\n",
      "Epoch: 185, Train loss: 0.629, Val loss: 0.990, Epoch time = 92.995s\n",
      "Epoch: 186, Train loss: 0.627, Val loss: 0.984, Epoch time = 93.040s\n",
      "Epoch: 187, Train loss: 0.626, Val loss: 0.986, Epoch time = 93.206s\n",
      "Epoch: 188, Train loss: 0.624, Val loss: 0.990, Epoch time = 92.956s\n",
      "Epoch: 189, Train loss: 0.624, Val loss: 0.992, Epoch time = 93.000s\n",
      "Epoch: 190, Train loss: 0.621, Val loss: 0.985, Epoch time = 93.065s\n",
      "Epoch: 191, Train loss: 0.619, Val loss: 0.987, Epoch time = 92.909s\n",
      "Epoch: 192, Train loss: 0.618, Val loss: 0.986, Epoch time = 93.316s\n",
      "Epoch: 193, Train loss: 0.617, Val loss: 0.980, Epoch time = 93.127s\n",
      "Epoch: 194, Train loss: 0.614, Val loss: 0.986, Epoch time = 93.196s\n",
      "Epoch: 195, Train loss: 0.614, Val loss: 0.984, Epoch time = 93.027s\n",
      "Epoch: 196, Train loss: 0.612, Val loss: 0.985, Epoch time = 93.042s\n",
      "Epoch: 197, Train loss: 0.610, Val loss: 0.985, Epoch time = 93.272s\n",
      "Epoch: 198, Train loss: 0.609, Val loss: 0.984, Epoch time = 92.908s\n",
      "Epoch: 199, Train loss: 0.608, Val loss: 0.983, Epoch time = 93.342s\n",
      "Epoch: 200, Train loss: 0.607, Val loss: 0.985, Epoch time = 93.021s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12f4b93e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T19:28:32.254657Z",
     "iopub.status.busy": "2024-01-16T19:28:32.254371Z",
     "iopub.status.idle": "2024-01-16T19:28:32.363933Z",
     "shell.execute_reply": "2024-01-16T19:28:32.363116Z"
    },
    "papermill": {
     "duration": 0.144926,
     "end_time": "2024-01-16T19:28:32.366204",
     "exception": false,
     "start_time": "2024-01-16T19:28:32.221278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('outputs/translation_custom_dataloader', exist_ok=True)\n",
    "torch.save(model, 'outputs/translation_custom_dataloader/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79786a8",
   "metadata": {
    "papermill": {
     "duration": 0.031843,
     "end_time": "2024-01-16T19:28:32.430995",
     "exception": false,
     "start_time": "2024-01-16T19:28:32.399152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c3907d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T19:28:32.496010Z",
     "iopub.status.busy": "2024-01-16T19:28:32.495653Z",
     "iopub.status.idle": "2024-01-16T19:28:32.499851Z",
     "shell.execute_reply": "2024-01-16T19:28:32.499012Z"
    },
    "papermill": {
     "duration": 0.03884,
     "end_time": "2024-01-16T19:28:32.501810",
     "exception": false,
     "start_time": "2024-01-16T19:28:32.462970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from attention.transformer import TransformerDecoder, TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cd5779a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T19:28:32.567288Z",
     "iopub.status.busy": "2024-01-16T19:28:32.567013Z",
     "iopub.status.idle": "2024-01-16T19:28:32.619571Z",
     "shell.execute_reply": "2024-01-16T19:28:32.618757Z"
    },
    "papermill": {
     "duration": 0.087832,
     "end_time": "2024-01-16T19:28:32.621904",
     "exception": false,
     "start_time": "2024-01-16T19:28:32.534072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.load('outputs/translation_custom_dataloader/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7d80f31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T19:28:32.688293Z",
     "iopub.status.busy": "2024-01-16T19:28:32.687752Z",
     "iopub.status.idle": "2024-01-16T19:28:32.695464Z",
     "shell.execute_reply": "2024-01-16T19:28:32.694662Z"
    },
    "papermill": {
     "duration": 0.042569,
     "end_time": "2024-01-16T19:28:32.697375",
     "exception": false,
     "start_time": "2024-01-16T19:28:32.654806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_tgt_mask(tgt, pad_token_id=1):\n",
    "    \"\"\"\n",
    "    :param tgt: Target sequence.\n",
    "    Returns:\n",
    "        tgt_mask: Target mask.\n",
    "    \"\"\"\n",
    "    batch_size = tgt.shape[0]\n",
    "    device = tgt.device\n",
    "\n",
    "    # Same as src_mask but we additionally want to mask tokens from looking forward into the future tokens\n",
    "    # Note: wherever the mask value is true we want to attend to that token, otherwise we mask (ignore) it.\n",
    "    sequence_length = tgt.shape[1]  # trg_token_ids shape = (B, T) where T max trg token-sequence length\n",
    "    trg_padding_mask = (tgt != pad_token_id).view(batch_size, 1, 1, -1)  # shape = (B, 1, 1, T)\n",
    "    trg_no_look_forward_mask = torch.triu(torch.ones((1, 1, sequence_length, sequence_length), device=device) == 1).transpose(2, 3)\n",
    "\n",
    "    # logic AND operation (both padding mask and no-look-forward must be true to attend to a certain target token)\n",
    "    tgt_mask = trg_padding_mask & trg_no_look_forward_mask  # final shape = (B, 1, T, T)\n",
    "    return tgt_mask\n",
    "    \n",
    "def make_src_mask(src, pad_token_id=1):\n",
    "    \"\"\"\n",
    "    :param src: Source sequence.\n",
    "\n",
    "    Returns:\n",
    "        src_mask: Source mask.\n",
    "    \"\"\"\n",
    "    batch_size = src.shape[0]\n",
    "\n",
    "    # src_mask shape = (B, 1, 1, S) check out attention function in transformer_model.py where masks are applied\n",
    "    # src_mask only masks pad tokens as we want to ignore their representations (no information in there...)\n",
    "    src_mask = (src != pad_token_id).view(batch_size, 1, 1, -1)\n",
    "    return src_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "749035ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T19:28:32.762271Z",
     "iopub.status.busy": "2024-01-16T19:28:32.762010Z",
     "iopub.status.idle": "2024-01-16T19:28:32.855959Z",
     "shell.execute_reply": "2024-01-16T19:28:32.855138Z"
    },
    "papermill": {
     "duration": 0.128986,
     "end_time": "2024-01-16T19:28:32.858169",
     "exception": false,
     "start_time": "2024-01-16T19:28:32.729183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder = TransformerDecoder(\n",
    "            TGT_VOCAB_SIZE,\n",
    "            EMB_SIZE,\n",
    "            MAX_LEN,\n",
    "            NUM_ENCODER_LAYERS,\n",
    "            expansion_factor=4,\n",
    "            n_heads=NHEAD\n",
    "        ).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b840ae3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T19:28:32.924050Z",
     "iopub.status.busy": "2024-01-16T19:28:32.923717Z",
     "iopub.status.idle": "2024-01-16T19:28:32.932936Z",
     "shell.execute_reply": "2024-01-16T19:28:32.932077Z"
    },
    "papermill": {
     "duration": 0.044126,
     "end_time": "2024-01-16T19:28:32.934885",
     "exception": false,
     "start_time": "2024-01-16T19:28:32.890759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.load_state_dict(model.decoder.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46dd299f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T19:28:33.001143Z",
     "iopub.status.busy": "2024-01-16T19:28:33.000395Z",
     "iopub.status.idle": "2024-01-16T19:28:33.074432Z",
     "shell.execute_reply": "2024-01-16T19:28:33.073729Z"
    },
    "papermill": {
     "duration": 0.109266,
     "end_time": "2024-01-16T19:28:33.076445",
     "exception": false,
     "start_time": "2024-01-16T19:28:32.967179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(\n",
    "            MAX_LEN,\n",
    "            SRC_VOCAB_SIZE,\n",
    "            EMB_SIZE,\n",
    "            NUM_ENCODER_LAYERS,\n",
    "            expansion_factor=4,\n",
    "            n_heads=NHEAD\n",
    "        ).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36439888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T19:28:33.144436Z",
     "iopub.status.busy": "2024-01-16T19:28:33.143631Z",
     "iopub.status.idle": "2024-01-16T19:28:33.151142Z",
     "shell.execute_reply": "2024-01-16T19:28:33.150311Z"
    },
    "papermill": {
     "duration": 0.042583,
     "end_time": "2024-01-16T19:28:33.153025",
     "exception": false,
     "start_time": "2024-01-16T19:28:33.110442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embedding): Embedding(\n",
       "      (embed): Embedding(25319, 256)\n",
       "    )\n",
       "    (positional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (embedding): Embedding(\n",
       "      (embed): Embedding(15389, 256)\n",
       "    )\n",
       "    (postional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x DecoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (k): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (v): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (transformer_block): TransformerBlock(\n",
       "          (attention): MultiHeadAttention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (k): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (v): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=15389, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1654cfd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T19:28:33.219086Z",
     "iopub.status.busy": "2024-01-16T19:28:33.218809Z",
     "iopub.status.idle": "2024-01-16T19:28:33.226910Z",
     "shell.execute_reply": "2024-01-16T19:28:33.226065Z"
    },
    "papermill": {
     "duration": 0.043291,
     "end_time": "2024-01-16T19:28:33.228711",
     "exception": false,
     "start_time": "2024-01-16T19:28:33.185420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode(src, tgt):\n",
    "    \"\"\"\n",
    "    :param src: Encoder input\n",
    "    :param tgt: Decoder input\n",
    "\n",
    "    Returns:\n",
    "        out_labels: Final prediction sequence\n",
    "    \"\"\"\n",
    "    tgt_mask = make_tgt_mask(tgt).to(DEVICE)\n",
    "    src_mask = make_src_mask(src).to(DEVICE)\n",
    "    enc_out = encoder(src)\n",
    "    out_labels = []\n",
    "    batch_size, seq_len = src.shape[0], src.shape[1]\n",
    "    out = tgt\n",
    "    with torch.no_grad():\n",
    "        for i in range(seq_len):\n",
    "            if i != 0:\n",
    "                tgt = torch.tensor(out_labels, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
    "                # print(tgt)\n",
    "                out = decoder(torch.tensor(tgt).to(DEVICE), enc_out, src_mask, tgt_mask)\n",
    "            else:\n",
    "                out = decoder(out, enc_out, src_mask, tgt_mask)\n",
    "            out = out.reshape(-1, out.shape[-1])\n",
    "            num_of_trg_tokens = len(tgt[0])\n",
    "            out = out[num_of_trg_tokens-1::num_of_trg_tokens]\n",
    "            out = torch.argmax(out, dim=-1)\n",
    "            out_labels.append(out.item())\n",
    "            out = torch.unsqueeze(out, 0)\n",
    "        return out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b764b93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T19:28:33.294589Z",
     "iopub.status.busy": "2024-01-16T19:28:33.294319Z",
     "iopub.status.idle": "2024-01-16T19:28:33.371049Z",
     "shell.execute_reply": "2024-01-16T19:28:33.370118Z"
    },
    "papermill": {
     "duration": 0.112521,
     "end_time": "2024-01-16T19:28:33.373439",
     "exception": false,
     "start_time": "2024-01-16T19:28:33.260918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you have to come .  . \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26/2588931828.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = decoder(torch.tensor(tgt).to(DEVICE), enc_out, src_mask, tgt_mask)\n"
     ]
    }
   ],
   "source": [
    "# Full-stops are important for the model to perform well.\n",
    "src_sentence = \"Bonjour, comment vas-tu?\"\n",
    "start_symbol = BOS_IDX\n",
    "src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "num_tokens = src.shape[0]\n",
    "src = src.to(DEVICE)\n",
    "ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "out = decode(torch.ravel(src).unsqueeze(0), ys)\n",
    "print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(out))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d94873",
   "metadata": {
    "papermill": {
     "duration": 0.032721,
     "end_time": "2024-01-16T19:28:33.439219",
     "exception": false,
     "start_time": "2024-01-16T19:28:33.406498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20553.691725,
   "end_time": "2024-01-16T19:28:35.535303",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-16T13:46:01.843578",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
