{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7eaf129",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Find small single text files for **Language Modeling** experiements here ⬇️\n",
    "\n",
    "https://www.kaggle.com/datasets/sovitrath/text-generation-language-modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab67365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tiktoken\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from utils.text_gen import get_batch, train, validate, NLPDataset\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch import Tensor\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from attention.transformer_linear_decoder import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c113ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67a64d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 25 22:36:32 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   46C    P5    29W / 370W |    417MiB / 10009MiB |     11%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1265      G   /usr/lib/xorg/Xorg                 35MiB |\r\n",
      "|    0   N/A  N/A      1862      G   /usr/lib/xorg/Xorg                159MiB |\r\n",
      "|    0   N/A  N/A      1998      G   /usr/bin/gnome-shell               32MiB |\r\n",
      "|    0   N/A  N/A      3697      G   ...309105646604474703,262144      135MiB |\r\n",
      "|    0   N/A  N/A     21367      G   ...RendererForSitePerProcess       37MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb8a3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = 'outputs/text_gen_simple_dec_imdb' \n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f536148c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imdb_train.txt']\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = os.path.join('../../input', 'imdb_single_file', 'train')\n",
    "train_file = os.listdir(dataset_dir)\n",
    "print(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2fac3",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n",
    "Let's find the longest review in the entire training set. As this will also contain the <br> tags, we will take the average of that.\n",
    "\n",
    "We will pad the smaller sentences to this average length and truncate the longer sentences to the average length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0eb7e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words (possibly, without tokenization): 17566362 words\n"
     ]
    }
   ],
   "source": [
    "def find_longest_length(text_file_paths):\n",
    "    \"\"\"\n",
    "    Find the longest review length in the entire training set. \n",
    "\n",
    "    :param text_file_paths: List, containing all the text file paths.\n",
    "\n",
    "    Returns:\n",
    "        max_len: Longest review length.\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "    for path in text_file_paths:\n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "            corpus = [\n",
    "                word for word in text.split()\n",
    "            ]\n",
    "        if len(corpus) > max_length:\n",
    "            max_length = len(corpus)\n",
    "    return max_length\n",
    "\n",
    "\n",
    "file_paths = []\n",
    "file_paths.extend(glob.glob(os.path.join(\n",
    "    dataset_dir, '*.txt'\n",
    ")))\n",
    "longest_sentence_length = find_longest_length(file_paths)\n",
    "print(f\"Total words (possibly, without tokenization): {longest_sentence_length} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f94dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words to generate in a sentence.\n",
    "SEQUENCE_LENGTH = 1024\n",
    "NUM_WORDS = 50304  # Vocabulary size.\n",
    "\n",
    "# Batch size.\n",
    "BATCH_SIZE = 1\n",
    "VALID_SPLIT = 0.1\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcefd8",
   "metadata": {},
   "source": [
    "### Helper Functions \n",
    "\n",
    "A few helper functions to prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5beabba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_frequency(\n",
    "    text_file_paths, num_files, most_common=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a list of tuples of the following format,\n",
    "    [('ho', 2), ('hello', 1), (\"let's\", 1), ('go', 1)]\n",
    "    where the number represents the frequency of occurance of \n",
    "    the word in the entire dataset.\n",
    "\n",
    "    :param text_file_paths: List, containing all the text file paths.\n",
    "    :param most_common: Return these many top words from the dataset.\n",
    "        If `most_common` is None, return all. If `most_common` is 3,\n",
    "        returns the top 3 tuple pairs in the list.\n",
    "\n",
    "    Returns:\n",
    "        sorted_words: A list of tuple containing each word and it's\n",
    "        frequency of the format ('ho', 2), ('hello', 1), ...]\n",
    "    \"\"\"\n",
    "    # Add all the words in the entire dataset to `corpus` list.\n",
    "    corpus = []\n",
    "    for i, path in enumerate(text_file_paths):\n",
    "        if i+1 == num_files:\n",
    "            break\n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "            # Remove <br> tags.\n",
    "            text = re.sub('<[^>]+>+', '', text)\n",
    "            corpus.extend([\n",
    "                word for word in text.split()\n",
    "            ])\n",
    "    count_words = Counter(corpus)\n",
    "    # Create a dictionary with the most common word in the corpus \n",
    "    # at the beginning.\n",
    "    # `word_frequency` will be like \n",
    "    word_frequency = count_words.most_common(n=most_common) # Returns all as n is `None`.\n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3175f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2int(input_words, num_words):\n",
    "    \"\"\"\n",
    "    Create a dictionary of word to integer mapping for each unique word.\n",
    "\n",
    "    :param input_words: A list of tuples containing the words and \n",
    "        theiry frequency. Should be of the following format,\n",
    "        [('ho', 2), ('hello', 1), (\"let's\", 1), ('go', 1)]\n",
    "    :param num_words: Number of words to use from the `input_words` list \n",
    "        to create the mapping. If -1, use all words in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        int_mapping: A dictionary of word and a integer mapping as \n",
    "            key-value pair. Example, {'Hello,': 1, 'the': 2, 'let': 3}\n",
    "    \"\"\"\n",
    "\n",
    "    if num_words > -1:\n",
    "        int_mapping = {\n",
    "            w:i+1 for i, (w, c) in enumerate(input_words) \\\n",
    "                if i <= num_words - 1 # -1 to avoid getting (num_words + 1) integer mapping.\n",
    "        }\n",
    "    else:\n",
    "        int_mapping = {w:i+1 for i, (w, c) in enumerate(input_words)}\n",
    "    return int_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c839d68",
   "metadata": {},
   "source": [
    "### Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e4824c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a58440",
   "metadata": {},
   "source": [
    "## Prepare PyTorch Datasets and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ac014e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = os.path.join(dataset_dir, train_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c29007b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_inst = NLPDataset(file_paths, enc)\n",
    "dataset = dataset_inst.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1e7d42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: torch.Size([22667909])\n",
      "Number of unique tokens: 45163\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total tokens: {dataset.shape}\")\n",
    "print(f\"Number of unique tokens: {len(np.unique(dataset))}\")\n",
    "# print(f\"Number of chosen words to act as vocabulary (tokens): {len(int_mapping)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9def8720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 20401119\n",
      "Number of validation samples: 2266790\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "# Calculate the validation dataset size.\n",
    "valid_size = int(VALID_SPLIT*dataset_size)\n",
    "# Radomize the data indices.\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "# Training and validation sets.\n",
    "dataset_train = dataset[:-valid_size]\n",
    "dataset_valid = dataset[-valid_size:]\n",
    "\n",
    "# dataset_valid = NLPClassificationDataset()\n",
    "print(f\"Number of training samples: {len(dataset_train)}\")\n",
    "print(f\"Number of validation samples: {len(dataset_valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0361eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20401119\n",
      "2266790\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.size(0))\n",
    "print(dataset_valid.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afa95d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9e45605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(dataset_train):\n",
    "#     inp, tgt = get_batch('train')\n",
    "#     print(inp)\n",
    "#     print(tgt)\n",
    "#     inp_words = ''\n",
    "#     tgt_words = ''\n",
    "#     inp = inp[0].cpu().numpy()\n",
    "#     tgt = tgt[0].cpu().numpy()\n",
    "#     print(len(inp))\n",
    "#     print(len(tgt))\n",
    "#     for idx in inp:\n",
    "#         inp_words += ' ' + int2word_train[idx]\n",
    "#     print(inp_words)\n",
    "#     print('*'*50)\n",
    "#     for idx in tgt:\n",
    "#         tgt_words += ' ' + int2word_train[idx]\n",
    "#     print(tgt_words)\n",
    "#     if i == 2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66743536",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "513af03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    embed_dim=512, \n",
    "    src_vocab_size=NUM_WORDS, \n",
    "    seq_len=SEQUENCE_LENGTH,\n",
    "    num_layers=6, \n",
    "    expansion_factor=4, \n",
    "    n_heads=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5621ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508f234",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b96fd55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embedding): Embedding(\n",
      "      (embed): Embedding(50304, 512)\n",
      "    )\n",
      "    (positional_encoding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (3): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (4): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (5): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (k): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (v): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=50304, bias=True)\n",
      ")\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "65,822,976 total parameters.\n",
      "65,822,976 training parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=0.0001,\n",
    ")\n",
    "\n",
    "# StepLR every specific number of epochs.\n",
    "scheduler = StepLR(\n",
    "    optimizer, \n",
    "    step_size=5, \n",
    "    gamma=0.5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "999052b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Epoch 1 of 1\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557d0e45a00b46339dd021dfd428d1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO]: Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     train_epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mSEQUENCE_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mNUM_WORDS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     valid_epoch_loss \u001b[38;5;241m=\u001b[39m validate(\n\u001b[1;32m     19\u001b[0m         model, \n\u001b[1;32m     20\u001b[0m         dataset_valid,  \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m         device\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     27\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mappend(train_epoch_loss)\n",
      "File \u001b[0;32m/mnt/wwn-0x500a0751e6282b63-part2/my_data/Data_Science/Projects/NLP_Text_Sequence/attention_is_all_you_need/examples/utils/text_gen.py:38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataset_train, optimizer, criterion, sequence_length, vocab_size, batch_size, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, dataset_train\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), sequence_length), \n\u001b[1;32m     35\u001b[0m     total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(dataset_train\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39msequence_length)\n\u001b[1;32m     36\u001b[0m ):\n\u001b[1;32m     37\u001b[0m     counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 38\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Forward pass.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/wwn-0x500a0751e6282b63-part2/my_data/Data_Science/Projects/NLP_Text_Sequence/attention_is_all_you_need/examples/utils/text_gen.py:15\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(data, sequence_length, batch_size, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpin_memory()\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), y\u001b[38;5;241m.\u001b[39mpin_memory()\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = EPOCHS\n",
    "# Lists to keep track of losses and accuracies.\n",
    "train_loss, valid_loss = [], []\n",
    "train_acc, valid_acc = [], []\n",
    "# Start the training.\n",
    "for epoch in range(epochs):\n",
    "    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = train(\n",
    "        model, \n",
    "        dataset_train, \n",
    "        optimizer, \n",
    "        criterion,\n",
    "        SEQUENCE_LENGTH,\n",
    "        NUM_WORDS,\n",
    "        BATCH_SIZE,\n",
    "        device\n",
    "    )\n",
    "    valid_epoch_loss = validate(\n",
    "        model, \n",
    "        dataset_valid,  \n",
    "        criterion,\n",
    "        SEQUENCE_LENGTH,\n",
    "        NUM_WORDS,\n",
    "        BATCH_SIZE,\n",
    "        device\n",
    "    )\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "    print(f\"Training loss: {train_epoch_loss}\")\n",
    "    print(f\"Validation loss: {valid_epoch_loss}\")\n",
    "\n",
    "    # Save model.\n",
    "    torch.save(\n",
    "        model, os.path.join(CHECKPOINT_DIR, 'model.pth')\n",
    "    )\n",
    "    print('-'*50)\n",
    "#     if epoch + 1 <= 32:\n",
    "#         scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56e504f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(train_acc, valid_acc, train_loss, valid_loss):\n",
    "    \"\"\"\n",
    "    Function to save the loss and accuracy plots to disk.\n",
    "    \"\"\"\n",
    "    plt.show()\n",
    "    # Loss plots.\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(\n",
    "        train_loss, color='blue', linestyle='-', \n",
    "        label='train loss'\n",
    "    )\n",
    "    plt.plot(\n",
    "        valid_loss, color='red', linestyle='-', \n",
    "        label='validataion loss'\n",
    "    )\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "#     plt.savefig(f\"../outputs/loss.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5e4eced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm8AAAGpCAYAAADbb9G8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAAsTAAALEwEAmpwYAAAgRElEQVR4nO3df7RddX3n/9dbiIRfQohRkdAJjkxJAiHAFbCpAqIIUgGrFhRrdGn96tJSxjWWVFt/UO2ClioLi1qwWoa2UEqXShdaBhWknbGWhEEkohN+hBJQDAiRFFDBz/ePe6AhvSGXXO49+SSPx1p33bP3/px9PudukKf77HNOtdYCAEAfnjHsCQAAMH7iDQCgI+INAKAj4g0AoCPiDQCgI9sOewJT6dnPfnabM2fOsKcBALBRy5Ytu6e1Nmv99VtVvM2ZMydLly4d9jQAADaqqm4fa72XTQEAOiLeAAA6It4AADqyVV3zBgBbmp///OdZtWpVHn744WFPhU00ffr0zJ49O9OmTRvXePEGAB1btWpVdt5558yZMydVNezp8BS11nLvvfdm1apV2WuvvcZ1Hy+bAkDHHn744cycOVO4daqqMnPmzKd05lS8AUDnhFvfnurxE28AAB0RbwDAJrv//vvzqU99apPu+6pXvSr333//uMd/+MMfzllnnbVJj7UlEW8AwCZ7snh75JFHnvS+X/7yl7PrrrtOwqy2bOINANhkS5YsyS233JKFCxfmfe97X66++uq85CUvyXHHHZd58+YlSU444YQcdNBBmT9/fs4777zH7ztnzpzcc889WblyZebOnZvf+q3fyvz583PUUUfloYceetLHvf7663PooYdmwYIFec1rXpP77rsvSXLOOedk3rx5WbBgQU466aQkyTe+8Y0sXLgwCxcuzAEHHJAHHnhgkv4aU8NHhQDAFuLUU5Prr39697lwYXL22RvefsYZZ+TGG2/M9YMHvvrqq3PdddflxhtvfPyjLz73uc9lt912y0MPPZQXvehFee1rX5uZM2c+YT8rVqzIRRddlPPPPz+/8Ru/kb//+7/Pm970pg0+7pvf/OZ88pOfzGGHHZYPfvCD+chHPpKzzz47Z5xxRm677bZst912j78ke9ZZZ+Xcc8/NokWLsnbt2kyfPn0Cf5Hhc+YNAHhaHXzwwU/4zLJzzjkn+++/fw499NDccccdWbFixX+6z1577ZWFCxcmSQ466KCsXLlyg/tfs2ZN7r///hx22GFJksWLF+eaa65JkixYsCAnn3xy/uqv/irbbjt6jmrRokV573vfm3POOSf333//4+t71ffsAYDHPdkZsqm04447Pn776quvzle/+tV885vfzA477JDDDz98zM8022677R6/vc0222z0ZdMNufzyy3PNNdfkH/7hH/Kxj30s3/nOd7JkyZIce+yx+fKXv5xFixbliiuuyD777LNJ+98cOPMGAGyynXfe+UmvIVuzZk1mzJiRHXbYId/73vfyL//yLxN+zF122SUzZszIP/3TPyVJLrzwwhx22GH5xS9+kTvuuCNHHHFEzjzzzKxZsyZr167NLbfckv322y+nnXZaXvSiF+V73/vehOcwTM68AQCbbObMmVm0aFH23XffHHPMMTn22GOfsP3oo4/OZz7zmcydOze//Mu/nEMPPfRpedwLLrgg73znO/Pggw/mBS94QT7/+c/n0UcfzZve9KasWbMmrbWccsop2XXXXfMHf/AHueqqq/KMZzwj8+fPzzHHHPO0zGFYqrU27DlMmZGRkbZ06dJhTwMAnjY33XRT5s6dO+xpMEFjHceqWtZaG1l/rJdNAQA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAIAptdNOOyVJ7rrrrrzuda8bc8zhhx+ejX2819lnn50HH3xwo4/39re/Pd/97nef+kTX85d/+Zd5z3veM+H9TJR4AwCG4vnPf34uvfTSTb7/eOPts5/9bObNm7fJj7O5EW8AwCZbsmRJzj333MeXP/zhD+ess87K2rVrc+SRR+bAAw/Mfvvtly996Uv/6b4rV67MvvvumyR56KGHctJJJ2Xu3Ll5zWte84TvNn3Xu96VkZGRzJ8/Px/60IeSjH7Z/V133ZUjjjgiRxxxxAbHJU88i3fRRRdlv/32y7777pvTTjvt8TE77bRTPvCBD2T//ffPoYcemrvvvvtJn/fKlSvzspe9LAsWLMiRRx6Zf/u3f0uS/N3f/V323Xff7L///nnpS1+aJFm+fHkOPvjgLFy4MAsWLMiKFSvG/wceg6/HAoAtxamnJtdf//Tuc+HCJ/3G+xNPPDGnnnpq3v3udydJLrnkklxxxRWZPn16vvCFL+RZz3pW7rnnnhx66KE57rjjUlVj7ufTn/50dthhh9x000254YYbcuCBBz6+7WMf+1h22223PProoznyyCNzww035JRTTsnHP/7xXHXVVXn2s5+9wXELFix4fD933XVXTjvttCxbtiwzZszIUUcdlS9+8Ys54YQT8u///u859NBD87GPfSy/+7u/m/PPPz+///u/v8Hn/du//dtZvHhxFi9enM997nM55ZRT8sUvfjGnn356rrjiiuyxxx65//77kySf+cxn8ju/8zs5+eST87Of/SyPPvroOP/4Y3PmDQDYZAcccEB+9KMf5a677sq3v/3tzJgxI3vuuWdaa3n/+9+fBQsW5OUvf3nuvPPOJz2bdc011+RNb3pTkmTBggVPiK5LLrkkBx54YA444IAsX758g9evbWzctddem8MPPzyzZs3Ktttum5NPPjnXXHNNkuSZz3xmfu3Xfi1JctBBB2XlypVP+ry/+c1v5o1vfGOS5Dd/8zfzz//8z0mSRYsW5S1veUvOP//8xyPtxS9+cf7oj/4oZ555Zm6//fZsv/32T7rvjXHmDQC2FE9yhmwyvf71r8+ll16aH/7whznxxBOTJH/913+d1atXZ9myZZk2bVrmzJmThx9++Cnv+7bbbstZZ52Va6+9NjNmzMhb3vKWMfcz3nEbMm3atMfPCm6zzTZ55JFHnvJck9GzbN/61rdy+eWX56CDDsqyZcvyxje+MYccckguv/zyvOpVr8qf//mf52Uve9km7T9x5g0AmKATTzwxF198cS699NK8/vWvT5KsWbMmz3nOczJt2rRcddVVuf322590Hy996UvzN3/zN0mSG2+8MTfccEOS5Cc/+Ul23HHH7LLLLrn77rvzla985fH77LzzznnggQc2Ou4xBx98cL7xjW/knnvuyaOPPpqLLroohx122CY951/5lV/JxRdfnGQ0VF/ykpckSW655ZYccsghOf300zNr1qzccccdufXWW/OCF7wgp5xySo4//vjHn9umcuYNAJiQ+fPn54EHHsgee+yR3XffPUly8skn59WvfnX222+/jIyMZJ999nnSfbzrXe/KW9/61sydOzdz587NQQcdlCTZf//9c8ABB2SfffbJnnvumUWLFj1+n3e84x05+uij8/znPz9XXXXVBsc9Zvfdd88ZZ5yRI444Iq21HHvssTn++OM36Tl/8pOfzFvf+tb8yZ/8SWbNmpXPf/7zSZL3ve99WbFiRVprOfLII7P//vvnzDPPzIUXXphp06blec97Xt7//vdv0mM+plprE9pBT0ZGRtrGPjMGAHpy0003Ze7cucOeBhM01nGsqmWttZH1x3rZFACgI+INAKAj4g0AOrc1XQK1JXqqx0+8AUDHpk+fnnvvvVfAdaq1lnvvvTfTp08f93282xQAOjZ79uysWrUqq1evHvZU2ETTp0/P7Nmzxz1evAFAx6ZNm5a99tpr2NNgCnnZFACgI+INAKAj4g0AoCPiDQCgI+INAKAj4g0AoCPiDQCgI+INAKAj4g0AoCNDjbeqOrqqvl9VN1fVkjG2b1dVfzvY/q2qmrPe9l+qqrVV9T+mbNIAAEM0tHirqm2SnJvkmCTzkryhquatN+xtSe5rrb0wySeSnLne9o8n+cpkzxUAYHMxzDNvBye5ubV2a2vtZ0kuTnL8emOOT3LB4PalSY6sqkqSqjohyW1Jlk/NdAEAhm+Y8bZHkjvWWV41WDfmmNbaI0nWJJlZVTslOS3JRzb2IFX1jqpaWlVLV69e/bRMHABgWHp9w8KHk3yitbZ2YwNba+e11kZaayOzZs2a/JkBAEyibYf42Hcm2XOd5dmDdWONWVVV2ybZJcm9SQ5J8rqq+uMkuyb5RVU93Fr7s0mfNQDAEA0z3q5NsndV7ZXRSDspyRvXG3NZksVJvpnkdUm+3lprSV7y2ICq+nCStcINANgaDC3eWmuPVNV7klyRZJskn2utLa+q05Msba1dluQvklxYVTcn+XFGAw8AYKtVoyeytg4jIyNt6dKlw54GAMBGVdWy1trI+ut7fcMCAMBWSbwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdES8AQB0RLwBAHREvAEAdGSo8VZVR1fV96vq5qpaMsb27arqbwfbv1VVcwbrX1FVy6rqO4PfL5vyyQMADMHQ4q2qtklybpJjksxL8oaqmrfesLclua+19sIkn0hy5mD9PUle3VrbL8niJBdOzawBAIZrmGfeDk5yc2vt1tbaz5JcnOT49cYcn+SCwe1LkxxZVdVa+7+ttbsG65cn2b6qtpuSWQMADNEw422PJHess7xqsG7MMa21R5KsSTJzvTGvTXJda+2nYz1IVb2jqpZW1dLVq1c/LRMHABiWrt+wUFXzM/pS6v+3oTGttfNaayOttZFZs2ZN3eQAACbBMOPtziR7rrM8e7BuzDFVtW2SXZLcO1ieneQLSd7cWrtl0mcLALAZGGa8XZtk76raq6qemeSkJJetN+ayjL4hIUlel+TrrbVWVbsmuTzJktba/56qCQMADNvQ4m1wDdt7klyR5KYkl7TWllfV6VV13GDYXySZWVU3J3lvksc+TuQ9SV6Y5INVdf3g5zlT/BQAAKZctdaGPYcpMzIy0pYuXTrsaQAAbFRVLWutjay/vus3LAAAbG3EGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBsAQEfGFW9VtWNVPWNw+79V1XFVNW1ypwYAwPrGe+btmiTTq2qPJP8ryW8m+cvJmhQAAGMbb7xVa+3BJL+e5FOttdcnmT950wIAYCzjjreqenGSk5NcPli3zeRMCQCADRlvvJ2a5PeSfKG1tryqXpDkqkmbFQAAYxpXvLXWvtFaO661dubgjQv3tNZOmeiDV9XRVfX9qrq5qpaMsX27qvrbwfZvVdWcdbb93mD996vqlROdCwBAD8b7btO/qapnVdWOSW5M8t2qet9EHriqtklybpJjksxL8oaqmrfesLclua+19sIkn0hy5uC+85KclNHr7o5O8qnB/gAAtmjjfdl0XmvtJ0lOSPKVJHtl9B2nE3Fwkptba7e21n6W5OIkx6835vgkFwxuX5rkyKqqwfqLW2s/ba3dluTmwf4AALZo4423aYPPdTshyWWttZ8naRN87D2S3LHO8qrBujHHtNYeSbImycxx3hcAYIsz3nj78yQrk+yY5Jqq+i9JfjJZk3o6VdU7qmppVS1dvXr1sKcDADAh433DwjmttT1aa69qo25PcsQEH/vOJHuuszx7sG7MMVW1bZJdktw7zvs+NvfzWmsjrbWRWbNmTXDKAADDNd43LOxSVR9/7AxWVf1pRs/CTcS1Sfauqr2q6pkZfQPCZeuNuSzJ4sHt1yX5emutDdafNHg36l5J9k7yrxOcDwDAZm+8L5t+LskDSX5j8POTJJ+fyAMPrmF7T5IrktyU5JLBZ8idXlXHDYb9RZKZVXVzkvcmWTK47/IklyT5bpJ/TPLu1tqjE5kPAEAPavRE1kYGVV3fWlu4sXWbu5GRkbZ06dJhTwMAYKOqallrbWT99eM98/ZQVf3qOjtblOShp2tyAACMz7bjHPfOJP+zqnYZLN+X/7gWDQCAKTKueGutfTvJ/lX1rMHyT6rq1CQ3TOLcAABYz3hfNk0yGm2Db1pIRt9AAADAFHpK8baeetpmAQDAuEwk3ib69VgAADxFT3rNW1U9kLEjrZJsPykzAgBgg5403lprO0/VRAAA2LiJvGwKAMAUE28AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0ZSrxV1W5VdWVVrRj8nrGBcYsHY1ZU1eLBuh2q6vKq+l5VLa+qM6Z29gAAwzOsM29LknyttbZ3kq8Nlp+gqnZL8qEkhyQ5OMmH1om8s1pr+yQ5IMmiqjpmaqYNADBcw4q345NcMLh9QZITxhjzyiRXttZ+3Fq7L8mVSY5urT3YWrsqSVprP0tyXZLZkz9lAIDhG1a8Pbe19oPB7R8mee4YY/ZIcsc6y6sG6x5XVbsmeXVGz96NqareUVVLq2rp6tWrJzRpAIBh23aydlxVX03yvDE2fWDdhdZaq6q2CfvfNslFSc5prd26oXGttfOSnJckIyMjT/lxAAA2J5MWb621l29oW1XdXVW7t9Z+UFW7J/nRGMPuTHL4Osuzk1y9zvJ5SVa01s6e+GwBAPowrJdNL0uyeHB7cZIvjTHmiiRHVdWMwRsVjhqsS1V9NMkuSU6d/KkCAGw+hhVvZyR5RVWtSPLywXKqaqSqPpskrbUfJ/nDJNcOfk5vrf24qmZn9KXXeUmuq6rrq+rtw3gSAABTrVrbei4DGxkZaUuXLh32NAAANqqqlrXWRtZf7xsWAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOiLeAAA6It4AADoi3gAAOjKUeKuq3arqyqpaMfg9YwPjFg/GrKiqxWNsv6yqbpz8GQMAbB6GdeZtSZKvtdb2TvK1wfITVNVuST6U5JAkByf50LqRV1W/nmTt1EwXAGDzMKx4Oz7JBYPbFyQ5YYwxr0xyZWvtx621+5JcmeToJKmqnZK8N8lHJ3+qAACbj2HF23Nbaz8Y3P5hkueOMWaPJHess7xqsC5J/jDJnyZ5cGMPVFXvqKqlVbV09erVE5gyAMDwbTtZO66qryZ53hibPrDuQmutVVV7CvtdmOS/ttb+e1XN2dj41tp5Sc5LkpGRkXE/DgDA5mjS4q219vINbauqu6tq99baD6pq9yQ/GmPYnUkOX2d5dpKrk7w4yUhVrczo/J9TVVe31g4PAMAWblgvm16W5LF3jy5O8qUxxlyR5KiqmjF4o8JRSa5orX26tfb81tqcJL+a5P8JNwBgazGseDsjySuqakWSlw+WU1UjVfXZJGmt/Tij17ZdO/g5fbAOAGCrVa1tPZeBjYyMtKVLlw57GgAAG1VVy1prI+uv9w0LAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB0RbwAAHRFvAAAdqdbasOcwZapqdZLbhz2Pjjw7yT3DngRP4JhsnhyXzY9jsnlyXJ6a/9Jam7X+yq0q3nhqqmppa21k2PPgPzgmmyfHZfPjmGyeHJenh5dNAQA6It4AADoi3ngy5w17AvwnjsnmyXHZ/DgmmyfH5WngmjcAgI448wYA0BHxBgDQEfG2lauq3arqyqpaMfg9YwPjFg/GrKiqxWNsv6yqbpz8GW/5JnJMqmqHqrq8qr5XVcur6oypnf2WpaqOrqrvV9XNVbVkjO3bVdXfDrZ/q6rmrLPt9wbrv19Vr5zSiW/hNvW4VNUrqmpZVX1n8PtlUz75LdRE/l0ZbP+lqlpbVf9jyibdMfHGkiRfa63tneRrg+UnqKrdknwoySFJDk7yoXWDoqp+PcnaqZnuVmGix+Ss1to+SQ5IsqiqjpmaaW9ZqmqbJOcmOSbJvCRvqKp56w17W5L7WmsvTPKJJGcO7jsvyUlJ5ic5OsmnBvtjgiZyXDL64bCvbq3tl2RxkgunZtZbtgkek8d8PMlXJnuuWwrxxvFJLhjcviDJCWOMeWWSK1trP26t3Zfkyoz+BylVtVOS9yb56ORPdauxycektfZga+2qJGmt/SzJdUlmT/6Ut0gHJ7m5tXbr4G95cUaPzbrWPVaXJjmyqmqw/uLW2k9ba7cluXmwPyZuk49La+3/ttbuGqxfnmT7qtpuSma9ZZvIvyupqhOS3JbRY8I4iDee21r7weD2D5M8d4wxeyS5Y53lVYN1SfKHSf40yYOTNsOtz0SPSZKkqnZN8uqMnr3jqdvo33jdMa21R5KsSTJznPdl00zkuKzrtUmua639dJLmuTXZ5GMyOAFwWpKPTME8txjbDnsCTL6q+mqS542x6QPrLrTWWlWN+7Njqmphkv/aWvvv61+/wJObrGOyzv63TXJRknNaa7du2ixhy1RV8zP6st1Rw54L+XCST7TW1g5OxDEO4m0r0Fp7+Ya2VdXdVbV7a+0HVbV7kh+NMezOJIevszw7ydVJXpxkpKpWZvSfpedU1dWttcPDk5rEY/KY85KsaK2dPfHZbrXuTLLnOsuzB+vGGrNqEMy7JLl3nPdl00zkuKSqZif5QpI3t9ZumfzpbhUmckwOSfK6qvrjJLsm+UVVPdxa+7NJn3XHvGzKZRm9cDeD318aY8wVSY6qqhmDi+KPSnJFa+3TrbXnt9bmJPnVJP9PuD0tNvmYJElVfTSj/8N46uRPdYt2bZK9q2qvqnpmRt+AcNl6Y9Y9Vq9L8vU2+snnlyU5afAOu72S7J3kX6do3lu6TT4ug0sJLk+ypLX2v6dqwluBTT4mrbWXtNbmDP47cnaSPxJuGyfeOCPJK6pqRZKXD5ZTVSNV9dkkaa39OKPXtl07+Dl9sI7JscnHZHBW4QMZfcfXdVV1fVW9fRhPoneD63Lek9EovinJJa215VV1elUdNxj2Fxm9bufmjL5xZ8ngvsuTXJLku0n+Mcm7W2uPTvVz2BJN5LgM7vfCJB8c/LtxfVU9Z4qfwhZngseETeDrsQAAOuLMGwBAR8QbAEBHxBsAQEfEGwBAR8QbAEBHxBuwVauqR9f52Ijrq+pp+wiDqppTVTc+XfsDSHzDAsBDrbWFw54EwHg58wYwhqpaWVV/XFXfqap/raoXDtbPqaqvV9UNVfW1qvqlwfrnVtUXqurbg59fGexqm6o6v6qWV9X/qqrtB+NPqarvDvZz8ZCeJtAh8QZs7bZf72XTE9fZtqa1tl+SP8voV/ckySeTXNBaW5Dkr5OcM1h/TpJvtNb2T3JgkuWD9XsnObe1Nj/J/UleO1i/JMkBg/28c3KeGrAl8g0LwFatqta21nYaY/3KJC9rrd1aVdOS/LC1NrOq7kmye2vt54P1P2itPbuqVieZ3Vr76Tr7mJPkytba3oPl05JMa619tKr+McnaJF9M8sXW2tpJfqrAFsKZN4ANaxu4/VT8dJ3bj+Y/rjU+Nsm5GT1Ld21VuQYZGBfxBrBhJ67z+5uD2/8nyUmD2ycn+afB7a8leVeSVNU2VbXLhnZaVc9Ismdr7aokpyXZJcl/OvsHMBb/Tw/Y2m1fVdevs/yPrbXHPi5kRlXdkNGzZ28YrPvtJJ+vqvclWZ3krYP1v5PkvKp6W0bPsL0ryQ828JjbJPmrQeBVknNaa/c/Tc8H2MK55g1gDINr3kZaa/cMey4A6/KyKQBAR5x5AwDoiDNvAAAdEW8AAB0RbwAAHRFvAAAdEW8AAB35/wFW/HRzK/tNpwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_plots(train_acc, valid_acc, train_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee6468b",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c3810a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'outputs/text_gen_nn_enc_simple_dec_imdb/model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHECKPOINT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.9/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.9/site-packages/torch/serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.9/site-packages/torch/serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'outputs/text_gen_nn_enc_simple_dec_imdb/model.pth'"
     ]
    }
   ],
   "source": [
    "trained_model = torch.load(\n",
    "    os.path.join(CHECKPOINT_DIR, 'model.pth')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss = validate(\n",
    "#     trained_model, \n",
    "#     dataset_test,  \n",
    "#     criterion, \n",
    "#     device\n",
    "# )\n",
    "\n",
    "# print(f\"Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e1a2f",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_int_vector(enc, text):\n",
    "        \"\"\"\n",
    "        Assign an integer to each word and return the integers in a list.\n",
    "        \"\"\"\n",
    "        return enc.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next(predictions, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Implement variable-temperature sampling from a probability\n",
    "    distribution.\n",
    "    \"\"\"\n",
    "    predictions = predictions.squeeze(0)[-1, :] / temperature\n",
    "    predictions = predictions.exp().cpu()\n",
    "    next_token = torch.multinomial(predictions, num_samples=1)\n",
    "    return int(next_token[0].cpu())\n",
    "    \n",
    "\n",
    "def text_generator(sentence, generate_length):\n",
    "    trained_model.eval()\n",
    "    temperatures = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] \n",
    "    num_tokens = len(sentence)\n",
    "    for temeperature in temperatures:\n",
    "        sample = sentence\n",
    "        print(f\"GENERATED SENTENCE WITH TEMPERATURE {temeperature}\")\n",
    "        for i in range(generate_length):\n",
    "            int_vector = return_int_vector(enc, sample)\n",
    "            input_tensor = torch.tensor(int_vector, dtype=torch.int32)\n",
    "            input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                predictions = trained_model(input_tensor)\n",
    "            next_token = sample_next(predictions)\n",
    "#             if next_token != 0: # Ignore <pad> index. Final sentence may be shorter.\n",
    "            sample += ' ' + enc.decode([next_token])\n",
    "        print(sample)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afbe177",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d418dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as Teachers\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9822d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    print(f\"PROMPT: {sentence}\")\n",
    "    text_generator(sentence, generate_length)\n",
    "    print('\\n############\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16e047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
